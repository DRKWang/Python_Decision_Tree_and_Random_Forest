{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f11a1f-12e3-47a0-8a0b-d42e3d1e6189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "newline = \"\\n\"\n",
    "max_tolerate_entropy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d294b4e8-b76f-41ef-8e31-bca64391fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# read text file into pandas DataFrame and \n",
    "# create header \n",
    "import os\n",
    "Column_names = [\"TIME_str\", \"TIME_sec\", \"latitude\", \"longitude\", \"ALTITUDE_FT\", \"GRD_TRK\", \"GRD_SPD\", \"VTC_SPD\", \\\n",
    "             \"Vertical_label\",\"Horizontal_label\", \"Speed_label\",\\\n",
    "            \"TURN_RATE\", \"REL_HEADING\", \"DIS_TO_DES\", \"MANEUVER_ID\" ]\n",
    "Selected_col_names = [\"GRD_SPD\", \"VTC_SPD\", \"DIS_TO_DES\"]\n",
    "Target_category_name = \"Vertical_label\"\n",
    "data_name = \"HOLD_3\"\n",
    "cwd = os.getcwd()\n",
    "mypath = cwd + f\"/../data/{data_name}/\"\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b74046-2ee5-48ae-9f93-8abb146a3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for file_name in onlyfiles:\n",
    "    if \"Traj\" in file_name:\n",
    "        if count == 0:\n",
    "            file_path = mypath + file_name\n",
    "            df = pd.read_csv(file_path, sep=\",\", header=None)\n",
    "            df.columns = Column_names\n",
    "            X_df = df[Selected_col_names]\n",
    "            Y_target = df[Target_category_name]\n",
    "        else:\n",
    "            file_path = mypath + file_name\n",
    "            df_temp = pd.read_csv(file_path, sep=\",\", header=None)\n",
    "            df_temp.columns = Column_names\n",
    "            \n",
    "            X_df_temp = df_temp[Selected_col_names]\n",
    "            Y_target_temp = df_temp[Target_category_name]\n",
    "            X_df = pd.concat([X_df, X_df_temp],ignore_index=True)\n",
    "            Y_target = pd.concat([Y_target, Y_target_temp])\n",
    "        count +=1\n",
    "        # if count >= 10:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cacb55b-1e94-481e-b878-bbbf5e6406d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'  Over 250 kts under 10000ft',\n",
       " ' Climb_Nominal',\n",
       " ' Descend_Nominal',\n",
       " ' MaintainCurrentAltitude',\n",
       " ' No Conclusion: Vertical'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names =  list(set(list(Y_target)))\n",
    "word_map_to_num = set(list(Y_target))\n",
    "print(\"category:\")\n",
    "word_map_to_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b6fa5e-ae32-42d0-9f77-ea2c02b8a0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of points: 54570\n"
     ]
    }
   ],
   "source": [
    "n_sample = len(X_df)\n",
    "print(\"number of points:\", n_sample)\n",
    "X_df = X_df.head(n_sample)\n",
    "Y_target = Y_target.head(n_sample)\n",
    "Y = []\n",
    "for y in Y_target:\n",
    "    for idx, s in enumerate(word_map_to_num):\n",
    "        if s == y:\n",
    "            Y.append(idx)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09de5645-1529-4d4d-b82f-1e5207334347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRD_SPD</th>\n",
       "      <th>VTC_SPD</th>\n",
       "      <th>DIS_TO_DES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>478.0</td>\n",
       "      <td>-960.0</td>\n",
       "      <td>158.983083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>478.0</td>\n",
       "      <td>-960.0</td>\n",
       "      <td>158.850864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>478.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>158.705733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>478.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>158.573513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>478.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>158.441294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54565</th>\n",
       "      <td>254.0</td>\n",
       "      <td>-640.0</td>\n",
       "      <td>0.070556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54566</th>\n",
       "      <td>254.0</td>\n",
       "      <td>-640.0</td>\n",
       "      <td>0.070556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54567</th>\n",
       "      <td>254.0</td>\n",
       "      <td>-640.0</td>\n",
       "      <td>0.070556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54568</th>\n",
       "      <td>254.0</td>\n",
       "      <td>-640.0</td>\n",
       "      <td>0.070556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54569</th>\n",
       "      <td>254.0</td>\n",
       "      <td>-640.0</td>\n",
       "      <td>0.070556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54570 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       GRD_SPD  VTC_SPD  DIS_TO_DES\n",
       "0        478.0   -960.0  158.983083\n",
       "1        478.0   -960.0  158.850864\n",
       "2        478.0  -1024.0  158.705733\n",
       "3        478.0  -1024.0  158.573513\n",
       "4        478.0  -1024.0  158.441294\n",
       "...        ...      ...         ...\n",
       "54565    254.0   -640.0    0.070556\n",
       "54566    254.0   -640.0    0.070556\n",
       "54567    254.0   -640.0    0.070556\n",
       "54568    254.0   -640.0    0.070556\n",
       "54569    254.0   -640.0    0.070556\n",
       "\n",
       "[54570 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3726cd2-9f25-4a86-afac-c92dd69b02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import dtreeviz\n",
    "from sklearn.tree import export_text\n",
    "# read text file into pandas DataFrame and \n",
    "# create header \n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# file_name_test = \"Traj_AAL80-11121269.dat\" # worst\n",
    "file_name_test = \"Traj_AAL245-11147798_L_HOLD.dat\" # normal\n",
    "\n",
    "file_path_test = mypath + file_name_test\n",
    "\n",
    "\n",
    "# file_name_test = \"Traj_AAL1252-11172342POS_HOLD.dat\"\n",
    "\n",
    "# file_path_test = cwd + \"/../data/HOLD/\" + file_name_test\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(file_path_test, sep=\",\", header=None)\n",
    "df_test.columns = Column_names\n",
    "X_df_test = df_test[Selected_col_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47fd8dfb-3c52-431e-b223-2ccb5f1e438e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of testing points: 3407\n"
     ]
    }
   ],
   "source": [
    "print(\"number of testing points:\", len(X_df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7af9570d-b187-441f-a55f-bdad3d36af33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3407"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_target = df_test[Target_category_name]\n",
    "test_sample = len(X_df_test)\n",
    "X_df_test = X_df_test.head(test_sample)\n",
    "Z_target = Z_target.head(test_sample)\n",
    "Z = []\n",
    "for z in Z_target:\n",
    "    for idx, s in enumerate(word_map_to_num):\n",
    "        if s == z:\n",
    "            Z.append(idx)\n",
    "Z = np.array(Z)\n",
    "len(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5b8eb3-5726-4b22-9685-583669609672",
   "metadata": {},
   "source": [
    "## target 1: vertical_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0156167d-9a8f-4064-bff9-8113ccd3b93a",
   "metadata": {},
   "source": [
    "# Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acb9d3ac-83b5-4e46-b902-6fc9234ba760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8629649990837457\n",
      "If VTC_SPD <= -160.00 && VTC_SPD <= -672.00 && DIS_TO_DES <= 96.90 && GRD_SPD <= 282.50 && GRD_SPD <= 174.50, then  class:  Descend_Nominal\n",
      "\n",
      "If VTC_SPD <= -160.00 && VTC_SPD <= -672.00 && DIS_TO_DES <= 96.90 && GRD_SPD <= 282.50 && GRD_SPD > 174.50, then  class:  Descend_Nominal\n",
      "\n",
      "If VTC_SPD <= -160.00 && VTC_SPD <= -672.00 && DIS_TO_DES <= 96.90 && GRD_SPD > 282.50 && GRD_SPD <= 316.50, then  class:  Descend_Nominal\n",
      "\n",
      "If VTC_SPD <= -160.00 && VTC_SPD <= -672.00 && DIS_TO_DES <= 96.90 && GRD_SPD > 282.50 && GRD_SPD > 316.50, then  class:  Descend_Nominal\n",
      "\n",
      "If VTC_SPD <= -160.00 && VTC_SPD <= -672.00 && DIS_TO_DES > 96.90 && VTC_SPD <= -928.00 && GRD_SPD <= 342.50, then  class:  Descend_Nominal\n",
      "\n",
      "If VTC_SPD <= -160.00 && VTC_SPD <= -672.00 && DIS_TO_DES > 96.90 && VTC_SPD <= -928.00 && GRD_SPD > 342.50, then  class:  Descend_Nominal\n",
      "\n",
      "If VTC_SPD <= -160.00 && VTC_SPD <= -672.00 && DIS_TO_DES > 96.90 && VTC_SPD > -928.00 && GRD_SPD <= 405.00, then  class:  Descend_Nominal\n",
      "\n",
      "If VTC_SPD <= -160.00 && VTC_SPD <= -672.00 && DIS_TO_DES > 96.90 && VTC_SPD > -928.00 && GRD_SPD > 405.00, then  class:  Descend_Nominal\n",
      "\n",
      "If VTC_SPD <= -160.00 && VTC_SPD > -672.00 && VTC_SPD <= -416.00 && GRD_SPD <= 308.50 && GRD_SPD <= 280.00, then  class:  Descend_Nominal\n",
      "\n",
      "If VTC_SPD <= -160.00 && VTC_SPD > -672.00 && VTC_SPD <= -416.00 && GRD_SPD <= 308.50 && GRD_SPD > 280.00, then  class:  Descend_Nominal\n",
      "\n",
      "If VTC_SPD <= -160.00 && VTC_SPD > -672.00 && VTC_SPD <= -416.00 && GRD_SPD > 308.50 && DIS_TO_DES <= 177.02, then  class:  Descend_Nominal\n",
      "\n",
      "If VTC_SPD <= -160.00 && VTC_SPD > -672.00 && VTC_SPD <= -416.00 && GRD_SPD > 308.50 && DIS_TO_DES > 177.02, then  class:  Descend_Nominal\n",
      "\n",
      "If VTC_SPD <= -160.00 && VTC_SPD > -672.00 && VTC_SPD > -416.00 && GRD_SPD <= 268.50 && DIS_TO_DES <= 32.72, then  class:  Descend_Nominal\n",
      "\n",
      "If VTC_SPD <= -160.00 && VTC_SPD > -672.00 && VTC_SPD > -416.00 && GRD_SPD <= 268.50 && DIS_TO_DES > 32.72, then  class:  Descend_Nominal\n",
      "\n",
      "If VTC_SPD <= -160.00 && VTC_SPD > -672.00 && VTC_SPD > -416.00 && GRD_SPD > 268.50 && GRD_SPD <= 311.50, then  class:   Over 250 kts under 10000ft\n",
      "\n",
      "If VTC_SPD <= -160.00 && VTC_SPD > -672.00 && VTC_SPD > -416.00 && GRD_SPD > 268.50 && GRD_SPD > 311.50, then  class:  Descend_Nominal\n",
      "\n",
      "If VTC_SPD > -160.00 && DIS_TO_DES <= 132.07 && GRD_SPD <= 414.50 && GRD_SPD <= 0.50 && DIS_TO_DES <= 0.04, then  class:  No Conclusion: Vertical\n",
      "\n",
      "If VTC_SPD > -160.00 && DIS_TO_DES <= 132.07 && GRD_SPD <= 414.50 && GRD_SPD <= 0.50 && DIS_TO_DES > 0.04, then  class:  No Conclusion: Vertical\n",
      "\n",
      "If VTC_SPD > -160.00 && DIS_TO_DES <= 132.07 && GRD_SPD <= 414.50 && GRD_SPD > 0.50 && DIS_TO_DES <= 0.00, then  class:  MaintainCurrentAltitude\n",
      "\n",
      "If VTC_SPD > -160.00 && DIS_TO_DES <= 132.07 && GRD_SPD <= 414.50 && GRD_SPD > 0.50 && DIS_TO_DES > 0.00, then  class:  No Conclusion: Vertical\n",
      "\n",
      "If VTC_SPD > -160.00 && DIS_TO_DES <= 132.07 && GRD_SPD > 414.50 && GRD_SPD <= 521.00 && GRD_SPD <= 441.50, then  class:  MaintainCurrentAltitude\n",
      "\n",
      "If VTC_SPD > -160.00 && DIS_TO_DES <= 132.07 && GRD_SPD > 414.50 && GRD_SPD <= 521.00 && GRD_SPD > 441.50, then  class:  MaintainCurrentAltitude\n",
      "\n",
      "If VTC_SPD > -160.00 && DIS_TO_DES <= 132.07 && GRD_SPD > 414.50 && GRD_SPD > 521.00 && DIS_TO_DES <= 119.65, then  class:  No Conclusion: Vertical\n",
      "\n",
      "If VTC_SPD > -160.00 && DIS_TO_DES <= 132.07 && GRD_SPD > 414.50 && GRD_SPD > 521.00 && DIS_TO_DES > 119.65, then  class:  No Conclusion: Vertical\n",
      "\n",
      "If VTC_SPD > -160.00 && DIS_TO_DES > 132.07 && VTC_SPD <= 224.00 && DIS_TO_DES <= 456.90 && GRD_SPD <= 422.50, then  class:  MaintainCurrentAltitude\n",
      "\n",
      "If VTC_SPD > -160.00 && DIS_TO_DES > 132.07 && VTC_SPD <= 224.00 && DIS_TO_DES <= 456.90 && GRD_SPD > 422.50, then  class:  MaintainCurrentAltitude\n",
      "\n",
      "If VTC_SPD > -160.00 && DIS_TO_DES > 132.07 && VTC_SPD <= 224.00 && DIS_TO_DES > 456.90 && DIS_TO_DES <= 515.53, then  class:  No Conclusion: Vertical\n",
      "\n",
      "If VTC_SPD > -160.00 && DIS_TO_DES > 132.07 && VTC_SPD <= 224.00 && DIS_TO_DES > 456.90 && DIS_TO_DES > 515.53, then  class:  No Conclusion: Vertical\n",
      "\n",
      "If VTC_SPD > -160.00 && DIS_TO_DES > 132.07 && VTC_SPD > 224.00, then  class:  Climb_Nominal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entropy, small tree with train data\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.tree import export_text_in_conditions\n",
    "max_depth = 5\n",
    "criterion = \"entropy\"\n",
    "dtree_small_entropy = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, min_samples_leaf=20)   # (random_state=1234)\n",
    "model=dtree_small_entropy.fit(X_df,Y)\n",
    "res = np.array(model.predict(X_df))\n",
    "diff = res - Y\n",
    "accuracy = 1/len(res) * sum([1 for x in diff if x == 0])\n",
    "print(accuracy)\n",
    "r = export_text_in_conditions(dtree_small_entropy, feature_names=list(X_df.columns), class_names=target_names, show_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeb120e9-709b-448c-aa03-0a61bf38d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {\n",
    "    \"GRD_SPD\": [\"very low\", \"low\", \"normal\", \"high\", \"very high\"],\n",
    "    \"VTC_SPD\": [\"UGTDesc\",\"OffNomDesc\", \"NomDesc\", \"NoClimb\", \"NomClimb\",\"OffNomClimb\", \"UGTClimb\"],\n",
    "    \"TURN_RATE\": [\"very low\", \"low\", \"normal\", \"high\", \"very high\"],\n",
    "    \"REL_HEADING\": [\"extremely left\", \"left\", \"straight\", \"right\", \"extremely right\"],\n",
    "    \"DIS_TO_DES\": [\"very close\", \"close\", \"a little far\", \"far\", \"very far\"]\n",
    "}\n",
    "pre_def_ranges = {\n",
    "    \"VTC_SPD\": [-2500, -2100, -1800, 0, 1650, 1800, 1950],\n",
    "\n",
    "}\n",
    "# five words needs 4 landmarks\n",
    "# seperate equally\n",
    "leaf_explainations = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "395e9be3-3177-4357-a616-6485df8f4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text_in_words\n",
    "from sklearn.utils.validation import check_array , check_is_fitted\n",
    "from sklearn.base import is_classifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, _criterion, _tree\n",
    "import matplotlib.pyplot as plt\n",
    "def compute_entropy(value):\n",
    "    rtn = 0\n",
    "    tot = np.sum(value)\n",
    "    probs = [v/tot for v in value]\n",
    "    for p in probs:\n",
    "        if p>= 0.001:\n",
    "            rtn += - p * np.log(p)\n",
    "    return rtn\n",
    "        \n",
    "def export_text_for_main_reasons(\n",
    "        X_df,\n",
    "        decision_tree,\n",
    "        *,\n",
    "        feature_names=None,\n",
    "        class_names=None,\n",
    "        max_depth=10,\n",
    "        spacing=3,\n",
    "        decimals=2,\n",
    "        show_weights=False,\n",
    "        show_prob = True,\n",
    "        crn_threshold = 0.1\n",
    "):\n",
    "\n",
    "    import copy\n",
    "    threshold_collection = dict()\n",
    "    if feature_names is not None:\n",
    "        feature_names = check_array(\n",
    "            feature_names, ensure_2d=False, dtype=None, ensure_min_samples=0\n",
    "        )\n",
    "    if class_names is not None:\n",
    "        class_names = check_array(\n",
    "            class_names, ensure_2d=False, dtype=None, ensure_min_samples=0\n",
    "        )\n",
    "\n",
    "    check_is_fitted(decision_tree)\n",
    "    tree_ = decision_tree.tree_\n",
    "    if is_classifier(decision_tree):\n",
    "        if class_names is None:\n",
    "            class_names = decision_tree.classes_\n",
    "        elif len(class_names) != len(decision_tree.classes_):\n",
    "            raise ValueError(\n",
    "                \"When `class_names` is an array, it should contain as\"\n",
    "                \" many items as `decision_tree.classes_`. Got\"\n",
    "                f\" {len(class_names)} while the tree was fitted with\"\n",
    "                f\" {len(decision_tree.classes_)} classes.\"\n",
    "            )\n",
    "    right_child_fmt = \"{} {} is {}\"\n",
    "    left_child_fmt = \"{} {} is {}\"\n",
    "    truncation_fmt = \"{} {}\\n\"\n",
    "\n",
    "    if feature_names is not None and len(feature_names) != tree_.n_features:\n",
    "        raise ValueError(\n",
    "            \"feature_names must contain %d elements, got %d\"\n",
    "            % (tree_.n_features, len(feature_names))\n",
    "        )\n",
    "\n",
    "    if isinstance(decision_tree, DecisionTreeClassifier):\n",
    "        value_fmt = \"{}{} weights: {}\\n\"\n",
    "        if not show_weights:\n",
    "            value_fmt = \"{}{}{}\\n\"\n",
    "    else:\n",
    "        value_fmt = \"{}{} value: {}\\n\"\n",
    "\n",
    "    if feature_names is not None:\n",
    "        feature_names_ = [\n",
    "            feature_names[i] if i != _tree.TREE_UNDEFINED else None\n",
    "            for i in tree_.feature\n",
    "        ]\n",
    "    else:\n",
    "        feature_names_ = [\"feature_{}\".format(i) for i in tree_.feature]\n",
    "\n",
    "    export_text.report = \"\"\n",
    "\n",
    "    def _add_leaf(node_id, value, class_name, indent, lower_bound_dict, upper_bound_dict, printable):\n",
    "        ## estiblish upper and lower dict for the features, upper collection the upper value, right child of the threshold, keep decreasing\n",
    "        # lower_bound_dict collect the lower value, left child of the threshold, keep increasing\n",
    "        #     right_child_fmt = \"{} {} <= {}\" left and right children are not corresponding to the graph, this is only for the tree,\n",
    "        #     left_child_fmt = \"{} {} > {}\" the tree is build from right small to left big\n",
    "        val = \"\"\n",
    "        is_classification = isinstance(decision_tree, DecisionTreeClassifier)\n",
    "        if show_weights or not is_classification:\n",
    "            val = [\"{1:.{0}f}, \".format(decimals, v) for v in value]\n",
    "            val = \"[\" + \"\".join(val)[:-2] + \"]\"\n",
    "        if is_classification:\n",
    "            val += \" class: \" + str(class_name)\n",
    "        export_text.report += value_fmt.format(indent, \"\", val)\n",
    "\n",
    "        ##\n",
    "        if printable:\n",
    "            output_conditions = []\n",
    "            all_feature_names = set(list(lower_bound_dict.keys()) + list(upper_bound_dict.keys()))\n",
    "            for key in all_feature_names:\n",
    "                cur_vocabulary = vocabulary[key]\n",
    "                interval_length = interval_length_collection[key]\n",
    "                cur_list = threshold_collection[key]\n",
    "                cur_words = []\n",
    "                if (key in lower_bound_dict.keys()) and (key in upper_bound_dict.keys()):\n",
    "                    for idx in range(len(cur_vocabulary)-1):\n",
    "                        left_range, right_range = cur_list[interval_length*idx], cur_list[interval_length*(idx+1)]\n",
    "                        if left_range <= lower_bound_dict[key] <= right_range or left_range <= upper_bound_dict[key] <= right_range:\n",
    "                            cur_words.append(cur_vocabulary[idx])\n",
    "                    right_range = cur_list[interval_length*(idx+1)]\n",
    "                    if lower_bound_dict[key] >= right_range or upper_bound_dict[key] >= right_range:\n",
    "                        cur_words.append(cur_vocabulary[-1])\n",
    "                    cur_range_txt = f\"(around {lower_bound_dict[key]:.02f} ~ {upper_bound_dict[key]:.02f})\"\n",
    "                    cur_condition = f\"{key} is \" + \" or \".join(cur_words) + cur_range_txt\n",
    "                    output_conditions.append(cur_condition)\n",
    "                    \n",
    "                elif (key in lower_bound_dict.keys()):\n",
    "                    for idx in range(len(cur_vocabulary)-1):\n",
    "                        left_range, right_range = cur_list[interval_length*idx], cur_list[interval_length*(idx+1)]\n",
    "                        if left_range <= lower_bound_dict[key] <= right_range:\n",
    "                            cur_words.append(cur_vocabulary[idx])\n",
    "                    right_range = cur_list[interval_length*(idx+1)]\n",
    "                    if lower_bound_dict[key] >= right_range:\n",
    "                        cur_words.append(cur_vocabulary[-1])\n",
    "                    cur_range_txt = f\"(> {lower_bound_dict[key]:.02f})\"\n",
    "                    cur_condition = f\"{key} is \" + \" or \".join(cur_words) + cur_range_txt\n",
    "                    output_conditions.append(cur_condition)\n",
    "\n",
    "                elif (key in upper_bound_dict.keys()):\n",
    "                    for idx in range(len(cur_vocabulary)-1):\n",
    "                        left_range, right_range = cur_list[interval_length*idx], cur_list[interval_length*(idx+1)]\n",
    "                        if left_range <= upper_bound_dict[key] <= right_range:\n",
    "                            cur_words.append(cur_vocabulary[idx])\n",
    "                    right_range = cur_list[interval_length*(idx+1)]\n",
    "                    if upper_bound_dict[key] >= right_range:\n",
    "                        cur_words.append(cur_vocabulary[-1])\n",
    "                    cur_range_txt = f\"(<= {upper_bound_dict[key]:.02f})\"\n",
    "                    cur_condition = f\"{key} is \" + \" or \".join(cur_words) + cur_range_txt\n",
    "                    output_conditions.append(cur_condition)\n",
    "            entropy = compute_entropy(value)\n",
    "            cur_number_of_node = np.sum(value)\n",
    "\n",
    "            CRN = entropy + (1 - cur_number_of_node/n_sample)\n",
    "            prob = np.max(value)/np.sum(value)*100\n",
    "            output_rst = \"Class: \" + f\"{str(class_name)}.\" + newline\n",
    "            output_conds = \"\"\n",
    "            for idx, cdn in enumerate(output_conditions):\n",
    "                if idx != len(output_conditions)-1:\n",
    "                    output_conds += cdn + \",\" + newline\n",
    "                else:\n",
    "                    output_conds += cdn + \".\" + newline\n",
    "            output_reasons = f\"CRN: {CRN:.2f},\" + newline\n",
    "            output_reasons += f\"Entropy (impurity): {entropy:.2f} | \"\n",
    "            output_reasons += f\"Prob: {cur_number_of_node / n_sample:.2f}({int(cur_number_of_node)} data).\" + newline\n",
    "            if CRN < crn_threshold:\n",
    "                global max_tolerate_entropy\n",
    "                # update max_tolerate_entropy \n",
    "                if max_tolerate_entropy < entropy:\n",
    "                    max_tolerate_entropy = entropy\n",
    "                final_output = output_rst + output_conds + output_reasons\n",
    "                leaf_explainations[node_id] = final_output\n",
    "                print(final_output)\n",
    "                print()\n",
    "    def print_tree_recurse(node, depth, lower_bound_dict, upper_bound_dict, printable):\n",
    "        indent = \"\"\n",
    "\n",
    "        value = None\n",
    "        if tree_.n_outputs == 1:\n",
    "            value = tree_.value[node][0]\n",
    "        else:\n",
    "            value = tree_.value[node].T[0]\n",
    "        class_name = np.argmax(value)\n",
    "\n",
    "        if tree_.n_classes[0] != 1 and tree_.n_outputs == 1:\n",
    "            class_name = class_names[class_name]\n",
    "\n",
    "        if depth <= max_depth + 1:\n",
    "            info_fmt = \"\"\n",
    "            info_fmt_left = info_fmt\n",
    "            info_fmt_right = info_fmt\n",
    "\n",
    "            if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "                name = feature_names_[node] # the name of the feature in string\n",
    "                threshold = tree_.threshold[node] # the value of the threshold\n",
    "                # threshold = \"{1:.{0}f}\".format(decimals, threshold)\n",
    "                if not printable:\n",
    "                    if name not in threshold_collection:\n",
    "                        threshold_collection[name] = []\n",
    "                    else:\n",
    "                        threshold_collection[name].append(threshold)\n",
    "\n",
    "\n",
    "                # print_tree_recurse(tree_.children_left[node], depth + 1, cur_pre_text + right_child_fmt.format(indent, name, threshold) )\n",
    "                # print the less than case, right_child_fmt is corresponding to the <= case.\n",
    "                changed_upper_bound_dict = copy.deepcopy(upper_bound_dict)\n",
    "                changed_upper_bound_dict[name]= threshold\n",
    "                print_tree_recurse(tree_.children_left[node], depth + 1, lower_bound_dict, changed_upper_bound_dict, printable)\n",
    "\n",
    "                changed_lower_bound_dict = copy.deepcopy(lower_bound_dict)\n",
    "                changed_lower_bound_dict[name]=threshold\n",
    "                print_tree_recurse(tree_.children_right[node], depth + 1, changed_lower_bound_dict, upper_bound_dict, printable)\n",
    "            else:  # leaf\n",
    "                _add_leaf(node, value, class_name, indent, lower_bound_dict, upper_bound_dict, printable)\n",
    "        else:\n",
    "            subtree_depth = _compute_depth(tree_, node)\n",
    "            if subtree_depth == 1:\n",
    "                _add_leaf(node, value, class_name, indent)\n",
    "            else:\n",
    "                trunc_report = \"truncated branch of depth %d\" % subtree_depth\n",
    "                export_text.report += truncation_fmt.format(indent, trunc_report)\n",
    "\n",
    "    upper_bound_dict = dict()\n",
    "    lower_bound_dict = dict()\n",
    "    # construct the threshold_collection\n",
    "    print_tree_recurse(0, 1, lower_bound_dict, upper_bound_dict, printable = False)\n",
    "    # print the range of levels for each feature\n",
    "    interval_length_collection = dict()\n",
    "    for feature_name in Selected_col_names:\n",
    "        threshold_collection[feature_name].sort()\n",
    "\n",
    "        cur_list = threshold_collection[feature_name]\n",
    "        cur_vocal = vocabulary[feature_name]\n",
    "        interval_length = (len(cur_list)-1) // (len(cur_vocal)-1)\n",
    "        interval_length_collection[feature_name] = interval_length\n",
    "        # threshold_collection[0:intervals_length] is the first level\n",
    "        # threshold_collection[intervals_length:2*intervals_length] is the second level\n",
    "        # ...\n",
    "        if feature_name in pre_def_ranges:\n",
    "            threshold_collection[feature_name] = pre_def_ranges[feature_name]\n",
    "            interval_length = 1\n",
    "            interval_length_collection[feature_name] = interval_length\n",
    "            cur_list = threshold_collection[feature_name]\n",
    "        else:\n",
    "            pre_def_ranges[feature_name] = [cur_list[interval_length*i] for i in range(len(cur_vocal))]\n",
    "            \n",
    "        print(f\"###########################################\")\n",
    "        print(f\"{r'feature name':^20} : {feature_name}\")\n",
    "        print(f\"{r'number of ranges':^20} : {len(cur_vocal)}\")\n",
    "        print(f\"-------------------------------------------\")\n",
    "        for i in range(len(cur_vocal)-1):\n",
    "            left_range = cur_list[interval_length*i]\n",
    "            right_range = cur_list[interval_length*(i+1)]\n",
    "            \n",
    "            print(f\"{cur_vocal[i]:^20} : [{left_range:.2f},{right_range:.2f}]\")\n",
    "        right_range = cur_list[interval_length*(i+1)]\n",
    "        print(f\"{cur_vocal[-1]:^20} :  >= {right_range:.2f}\")\n",
    "    print(f\"###########################################\")\n",
    "    upper_bound_dict = dict()\n",
    "    lower_bound_dict = dict()\n",
    "    print_tree_recurse(0, 1, lower_bound_dict, upper_bound_dict, printable = True)\n",
    "\n",
    "    # print(threshold_collection)\n",
    "    return export_text.report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5e20f92-853d-4a18-baec-12c1294c5a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "    feature name     : GRD_SPD\n",
      "  number of ranges   : 5\n",
      "-------------------------------------------\n",
      "      very low       : [0.50,280.00]\n",
      "        low          : [280.00,316.50]\n",
      "       normal        : [316.50,414.50]\n",
      "        high         : [414.50,521.00]\n",
      "     very high       :  >= 521.00\n",
      "###########################################\n",
      "    feature name     : VTC_SPD\n",
      "  number of ranges   : 7\n",
      "-------------------------------------------\n",
      "      UGTDesc        : [-2500.00,-2100.00]\n",
      "     OffNomDesc      : [-2100.00,-1800.00]\n",
      "      NomDesc        : [-1800.00,0.00]\n",
      "      NoClimb        : [0.00,1650.00]\n",
      "      NomClimb       : [1650.00,1800.00]\n",
      "    OffNomClimb      : [1800.00,1950.00]\n",
      "      UGTClimb       :  >= 1950.00\n",
      "###########################################\n",
      "    feature name     : DIS_TO_DES\n",
      "  number of ranges   : 5\n",
      "-------------------------------------------\n",
      "     very close      : [0.00,0.04]\n",
      "       close         : [0.04,32.72]\n",
      "    a little far     : [32.72,119.65]\n",
      "        far          : [119.65,132.07]\n",
      "      very far       :  >= 132.07\n",
      "###########################################\n",
      "Class:  Descend_Nominal.\n",
      "DIS_TO_DES is a little far(<= 96.90),\n",
      "GRD_SPD is very low(<= 174.50),\n",
      "VTC_SPD is NomDesc(<= -672.00).\n",
      "CRN: 0.98,\n",
      "Entropy (impurity): 0.00 | Prob: 0.02(1133 data).\n",
      "\n",
      "\n",
      "Class:  Descend_Nominal.\n",
      "DIS_TO_DES is a little far(> 96.90),\n",
      "GRD_SPD is normal(> 342.50),\n",
      "VTC_SPD is NomDesc(<= -928.00).\n",
      "CRN: 0.97,\n",
      "Entropy (impurity): 0.05 | Prob: 0.08(4195 data).\n",
      "\n",
      "\n",
      "Class:  No Conclusion: Vertical.\n",
      "DIS_TO_DES is very close or close or far or very far(around 0.04 ~ 132.07),\n",
      "GRD_SPD is very low(<= 0.50),\n",
      "VTC_SPD is NomDesc(> -160.00).\n",
      "CRN: 0.98,\n",
      "Entropy (impurity): 0.00 | Prob: 0.02(1161 data).\n",
      "\n",
      "\n",
      "Class:  MaintainCurrentAltitude.\n",
      "DIS_TO_DES is far or very far(<= 132.07),\n",
      "GRD_SPD is high or very high(around 441.50 ~ 521.00),\n",
      "VTC_SPD is NomDesc(> -160.00).\n",
      "CRN: 0.97,\n",
      "Entropy (impurity): 0.00 | Prob: 0.03(1478 data).\n",
      "\n",
      "\n",
      "Class:  No Conclusion: Vertical.\n",
      "DIS_TO_DES is a little far or far(<= 119.65),\n",
      "GRD_SPD is high or very high(> 521.00),\n",
      "VTC_SPD is NomDesc(> -160.00).\n",
      "CRN: 1.00,\n",
      "Entropy (impurity): 0.00 | Prob: 0.00(68 data).\n",
      "\n",
      "\n",
      "Class:  MaintainCurrentAltitude.\n",
      "DIS_TO_DES is far or very far(around 132.07 ~ 456.90),\n",
      "GRD_SPD is high(> 422.50),\n",
      "VTC_SPD is NomDesc or NoClimb(around -160.00 ~ 224.00).\n",
      "CRN: 0.91,\n",
      "Entropy (impurity): 0.11 | Prob: 0.20(11163 data).\n",
      "\n",
      "\n",
      "Class:  No Conclusion: Vertical.\n",
      "DIS_TO_DES is very far(around 456.90 ~ 515.53),\n",
      "VTC_SPD is NomDesc or NoClimb(around -160.00 ~ 224.00).\n",
      "CRN: 0.99,\n",
      "Entropy (impurity): 0.00 | Prob: 0.01(454 data).\n",
      "\n",
      "\n",
      "Class:  Climb_Nominal.\n",
      "DIS_TO_DES is far or very far(> 132.07),\n",
      "VTC_SPD is NoClimb(> 224.00).\n",
      "CRN: 0.99,\n",
      "Entropy (impurity): 0.00 | Prob: 0.01(677 data).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = export_text_for_main_reasons(X_df, dtree_small_entropy, feature_names=list(X_df.columns), class_names=target_names, show_weights=True, show_prob = True, crn_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4ba1424-062a-4c9e-afbe-aa9d6dc35203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11098074708066269"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tolerate_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be064e19-cd0f-4e06-a5eb-ef641c2c33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from typing import Mapping, List, Callable\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from colour import Color, rgb2hex\n",
    "from sklearn import tree\n",
    "\n",
    "from dtreeviz.colors import adjust_colors\n",
    "from dtreeviz.interpretation import explain_prediction_plain_english, explain_prediction_sklearn_default\n",
    "from dtreeviz.models.shadow_decision_tree import ShadowDecTree\n",
    "from dtreeviz.models.shadow_decision_tree import ShadowDecTreeNode\n",
    "from dtreeviz.utils import myround, DTreeVizRender, add_classifier_legend, _format_axes, _draw_wedge, \\\n",
    "                           _set_wedge_ticks, tessellate, is_numeric\n",
    "\n",
    "# How many bins should we have based upon number of classes\n",
    "NUM_BINS = [\n",
    "    0, 0, 10, 9, 8, 6,\n",
    "    6, 6, 5, 5, 5, 5,\n",
    "    5, 5, 5, 5, 5, 5,\n",
    "    5, 5, 5, 5, 5, 5,\n",
    "    5, 5, 5, 5, 5, 5,\n",
    "    5, 5, 5, 5, 5, 5,\n",
    "    5, 5, 5, 5, 5,\n",
    "]  # support for 40 classes\n",
    "\n",
    "\n",
    "class DTreeVizAPI:\n",
    "    \"\"\"\n",
    "    This object provides the primary interface to the functionality of this library. You can think of it as\n",
    "    an adaptor that adapts the various decision-tree based libraries for use with dtreeviz. In implementation,\n",
    "    however, this object encapsulates the key functionality and API but delegates tree model adaptation\n",
    "    from sklearn etc... to dtreeviz.models.ShadowDecTree subclasses.\n",
    "    \"\"\"\n",
    "    def __init__(self, shahdow_tree: ShadowDecTree):\n",
    "        self.shadow_tree = shahdow_tree\n",
    "\n",
    "    def leaf_sizes(self,\n",
    "                   display_type: str = \"plot\",\n",
    "                   colors: dict = None,\n",
    "                   fontsize: int = 10,\n",
    "                   fontname: str = \"Arial\",\n",
    "                   grid: bool = False,\n",
    "                   bins: int = 10,\n",
    "                   min_size: int = 0,\n",
    "                   max_size: int = None,\n",
    "                   figsize: tuple = None,\n",
    "                   ax=None):\n",
    "        \"\"\"Visualize leaf sizes.\n",
    "\n",
    "        Interpreting leaf sizes can help us to see how the data is spread over the tree:\n",
    "        - if we have a leaf with many samples and a good purity, it means that we can be pretty confident\n",
    "        on its prediction.\n",
    "        - if we have a leaf with few samples and a good purity, we cannot be very confident on its prediction and\n",
    "        it could be a sign of overfitting.\n",
    "        - by visualizing leaf sizes, we can easily discover important leaves . Using node_stats() function we\n",
    "        can take all of its samples and discover common patterns between leaf samples.\n",
    "        - if the tree contains a lot of leaves and we want a general overview about leaves sizes, we can use the\n",
    "        parameter display_type='hist' to display the histogram of leaf sizes.\n",
    "\n",
    "        There is the option to filter the leaves with sizes between 'min_size' and 'max_size'. This is helpful\n",
    "        especially when you want to investigate leaves with sizes from a specific range.\n",
    "\n",
    "        This method contains three types of visualizations:\n",
    "        - If display_type = 'plot' it will show leaf sizes using a plot.\n",
    "        - If display_type = 'text' it will show leaf sizes as plain text. This method is preferred if number\n",
    "        of leaves is very large and the plot become very big and hard to interpret.\n",
    "        - If display_type = 'hist' it will show leaf size histogram. Useful when you want to easily see the general\n",
    "        distribution of leaf sizes.\n",
    "\n",
    "        Usage example :\n",
    "        viz_model = dtreeviz.model(tree_model, X_train=dataset[features], y_train=dataset[target],\n",
    "                                   feature_names=features, target_name=target, class_names=[0, 1])\n",
    "        viz_model.leaf_sizes()\n",
    "\n",
    "        :param display_type: str, optional\n",
    "           'plot', 'text'. 'hist'\n",
    "        :param colors: dict\n",
    "            The set of colors used for plotting\n",
    "        :param fontsize: int\n",
    "            Plot labels font size\n",
    "        :param fontname: str\n",
    "            Plot labels font name\n",
    "        :param grid: bool\n",
    "            True if we want to display the grid lines on the visualization\n",
    "        :param bins: int\n",
    "            Number of histogram bins\n",
    "        :param min_size: int\n",
    "            Min size for a leaf\n",
    "        :param max_size: int\n",
    "            Max size for a leaf\n",
    "        :param figsize: optional (width, height) in inches for the entire plot\n",
    "            :param ax: optional matplotlib \"axes\" to draw into\n",
    "        \"\"\"\n",
    "        leaf_id, leaf_sizes = self.shadow_tree.get_leaf_sample_counts(min_size, max_size)\n",
    "\n",
    "        if display_type == \"text\":\n",
    "            for leaf, samples in zip(leaf_id, leaf_sizes):\n",
    "                print(f\"leaf {leaf} has {samples} samples\")\n",
    "        elif display_type in [\"plot\", \"hist\"]:\n",
    "            colors = adjust_colors(colors)\n",
    "            if ax is None:\n",
    "                if figsize:\n",
    "                    fig, ax = plt.subplots(figsize=figsize)\n",
    "                else:\n",
    "                    fig, ax = plt.subplots()\n",
    "        else:\n",
    "            raise ValueError(f'Unknown display_type = {display_type}')\n",
    "\n",
    "        if display_type == \"plot\":\n",
    "            ax.set_xticks(range(0, len(leaf_id)))\n",
    "            ax.set_xticklabels(leaf_id)\n",
    "            barcontainers = ax.bar(range(0, len(leaf_id)), leaf_sizes, color=colors[\"hist_bar\"], lw=.3,\n",
    "                                   align='center',\n",
    "                                   width=1)\n",
    "            for rect in barcontainers.patches:\n",
    "                rect.set_linewidth(.5)\n",
    "                rect.set_edgecolor(colors['rect_edge'])\n",
    "\n",
    "            _format_axes(ax, \"Leaf IDs\", \"Samples Count\", colors, fontsize, fontname, ticks_fontsize=None, grid=grid)\n",
    "\n",
    "        elif display_type == \"hist\":\n",
    "            n, bins, patches = ax.hist(leaf_sizes, bins=bins, color=colors[\"hist_bar\"])\n",
    "            for rect in patches:\n",
    "                rect.set_linewidth(.5)\n",
    "                rect.set_edgecolor(colors['rect_edge'])\n",
    "\n",
    "            _format_axes(ax, \"Leaf Sample\", \"Leaf Count\", colors, fontsize, fontname, ticks_fontsize=None, grid=grid)\n",
    "\n",
    "\n",
    "    def ctree_leaf_distributions(self,\n",
    "                                 display_type: (\"plot\", \"text\") = \"plot\",\n",
    "                                 xaxis_display_type: str = \"individual\",\n",
    "                                 show_leaf_id_list: list = None,\n",
    "                                 show_leaf_filter: Callable[[np.ndarray], bool] = None,\n",
    "                                 plot_ylim: int = None,\n",
    "                                 colors: dict = None,\n",
    "                                 fontsize: int = 10,\n",
    "                                 fontname: str = \"Arial\",\n",
    "                                 grid: bool = False,\n",
    "                                 figsize: tuple = None,\n",
    "                                 ax=None):\n",
    "        \"\"\"Visualize the distribution of classes for each leaf.\n",
    "\n",
    "        It's a good way to see how classes are distributed in leaves. For example, you can observe that in some\n",
    "        leaves all the samples belong only to one class, or that in other leaves the distribution of classes is almost\n",
    "        50/50.\n",
    "        You could get all the samples from these leaves (using node_stats() function) and look over/understand what they have in common.\n",
    "        Now, you can understand your data in a model driven way.\n",
    "\n",
    "        Usage example :\n",
    "        viz_model = dtreeviz.model(tree_model, X_train=dataset[features], y_train=dataset[target],\n",
    "                                   feature_names=features, target_name=target, class_names=[0, 1])\n",
    "        viz_model.ctree_leaf_distributions()\n",
    "\n",
    "        :param display_type: str, optional\n",
    "           'plot' or 'text'\n",
    "        :param xaxis_display_type: str, optional\n",
    "           'individual': Displays every node ID individually\n",
    "           'auto': Let matplotlib automatically manage the node ID ticks\n",
    "           'y_sorted': Display in y order with no x-axis tick labels\n",
    "        :param show_leaf_id_list: list, optional\n",
    "           The allowed list of node id values to plot\n",
    "        :param show_leaf_filter: Callable[[np.ndarray], bool], optional\n",
    "           The filtering function to apply to leaf values before displaying the leaves.\n",
    "           The function is applied to a numpy array with the class i sample value in row i.\n",
    "           For example, to view only those leaves with more than 100 total samples, and more than 5 class 1 samples, use show_leaf_filter = lambda x: (100 < np.sum(x)) & (5 < x[1])\n",
    "        :param plot_ylim: int, optional\n",
    "            The max value for oY. This is useful in case we have few leaves with big sample values which 'shadow'\n",
    "            the other leaves values.\n",
    "        :param colors: dict\n",
    "            The set of colors used for plotting\n",
    "        :param fontsize: int\n",
    "            Plot labels fontsize\n",
    "        :param fontname: str\n",
    "            Plot labels font name\n",
    "        :param grid: bool\n",
    "            True if we want to display the grid lines on the visualization\n",
    "        :param figsize: optional (width, height) in inches for the entire plot\n",
    "            :param ax: optional matplotlib \"axes\" to draw into\n",
    "        \"\"\"\n",
    "        index, leaf_samples = self.shadow_tree.get_leaf_sample_counts_by_class()\n",
    "        if display_type == \"plot\":\n",
    "            colors = adjust_colors(colors)\n",
    "            colors_classes = colors['classes'][self.shadow_tree.nclasses()]\n",
    "\n",
    "            if ax is None:\n",
    "                if figsize:\n",
    "                    fig, ax = plt.subplots(figsize=figsize)\n",
    "                else:\n",
    "                    fig, ax = plt.subplots()\n",
    "\n",
    "            leaf_samples_hist = [[] for i in range(self.shadow_tree.nclasses())]\n",
    "            for leaf_sample in leaf_samples:\n",
    "                for i, leaf_count in enumerate(leaf_sample):\n",
    "                    leaf_samples_hist[i].append(leaf_count)\n",
    "            leaf_samples_hist = np.array(leaf_samples_hist)\n",
    "\n",
    "            if show_leaf_id_list is not None:\n",
    "                _mask = np.isin(index, show_leaf_id_list)\n",
    "                leaf_samples_hist = leaf_samples_hist[:, _mask]\n",
    "                index = tuple(np.array(index)[_mask])\n",
    "            if show_leaf_filter is not None:\n",
    "                _mask = np.apply_along_axis(show_leaf_filter, 0, leaf_samples_hist)\n",
    "                leaf_samples_hist = leaf_samples_hist[:, _mask]\n",
    "                index = tuple(np.array(index)[_mask])\n",
    "\n",
    "            if xaxis_display_type == 'individual':\n",
    "                x = np.arange(0, len(index))\n",
    "                ax.set_xticks(x)\n",
    "                ax.set_xticklabels(index)\n",
    "            elif xaxis_display_type == 'auto':\n",
    "                x = np.array(index)\n",
    "                ax.set_xlim(np.min(x)-1, np.max(x)+1)\n",
    "            elif xaxis_display_type == 'y_sorted':\n",
    "                # sort by total y = sum(classes), then class 0, 1, 2, ...\n",
    "                sort_cols = [np.sum(leaf_samples_hist, axis=0)]\n",
    "                for i in range(leaf_samples_hist.shape[0]):\n",
    "                    sort_cols.append(leaf_samples_hist[i])\n",
    "                _sort = np.lexsort(sort_cols[::-1])[::-1]\n",
    "                leaf_samples_hist = leaf_samples_hist[:, _sort]\n",
    "                index = tuple(np.array(index)[_sort])\n",
    "\n",
    "                x = np.arange(0, len(index))\n",
    "                ax.set_xticks(x)\n",
    "                ax.set_xticklabels([])\n",
    "                ax.tick_params(axis='x', which='both', bottom=False)\n",
    "            else:\n",
    "                raise ValueError(f'Unknown xaxis_display_type = {xaxis_display_type}!')\n",
    "\n",
    "            if plot_ylim is not None:\n",
    "                ax.set_ylim(0, plot_ylim)\n",
    "\n",
    "            bottom_values = np.zeros(len(index))\n",
    "            for i in range(leaf_samples_hist.shape[0]):\n",
    "                bar_container = ax.bar(x, leaf_samples_hist[i], bottom=bottom_values,\n",
    "                                    color=colors_classes[i],\n",
    "                                    lw=.3, align='center', width=1)\n",
    "                bottom_values = bottom_values + leaf_samples_hist[i]\n",
    "\n",
    "                for rect in bar_container.patches:\n",
    "                    rect.set_linewidth(.5)\n",
    "                    rect.set_edgecolor(colors['rect_edge'])\n",
    "\n",
    "            class_values = self.shadow_tree.classes()\n",
    "            n_classes=self.shadow_tree.nclasses()\n",
    "            color_values = colors['classes'][n_classes]\n",
    "            color_map = {v: color_values[i] for i, v in enumerate(class_values)}\n",
    "            add_classifier_legend(ax, self.shadow_tree.class_names, class_values, color_map, self.shadow_tree.target_name, colors,\n",
    "                                fontname=fontname)\n",
    "\n",
    "            _format_axes(ax, \"Leaf IDs\", \"Samples by Class\", colors, fontsize, fontname, ticks_fontsize=None, grid=grid)\n",
    "\n",
    "        elif display_type == \"text\":\n",
    "            for i, leaf in enumerate(index):\n",
    "                print(f\"leaf {leaf}, samples : {leaf_samples[i]}\")\n",
    "\n",
    "    def view(self,\n",
    "             precision: int = 2,\n",
    "             orientation: ('TD', 'LR') = \"TD\",\n",
    "             instance_orientation: (\"TD\", \"LR\") = \"LR\",\n",
    "             show_root_edge_labels: bool = True,\n",
    "             show_node_labels: bool = False,\n",
    "             show_just_path: bool = False,\n",
    "             fancy: bool = True,\n",
    "             histtype: ('bar', 'barstacked', 'strip') = 'barstacked',\n",
    "             leaftype: ('pie', 'barh') = 'pie',\n",
    "             highlight_path: List[int] = [],\n",
    "             x: np.ndarray = None,\n",
    "             max_X_features_LR: int = 10,\n",
    "             max_X_features_TD: int = 20,\n",
    "             depth_range_to_display: tuple = None,\n",
    "             label_fontsize: int = 12,\n",
    "             ticks_fontsize: int = 8,\n",
    "             fontname: str = \"Arial\",\n",
    "             title: str = None,\n",
    "             title_fontsize: int = 10,\n",
    "             colors: dict = None,\n",
    "             scale=1.0\n",
    "             ) \\\n",
    "            -> DTreeVizRender:\n",
    "        \"\"\"\n",
    "        Based on a decision tree regressor or classifier, create and return a tree visualization using the\n",
    "        graphviz (DOT) language.\n",
    "\n",
    "        Usage example :\n",
    "        viz_model = dtreeviz.model(tree_model, X_train=dataset[features], y_train=dataset[target],\n",
    "                                   feature_names=features, target_name=target, class_names=[0, 1])\n",
    "        viz_model.view()\n",
    "\n",
    "        :param precision: When displaying floating-point numbers, how many digits to display\n",
    "                          after the decimal point. Default is 2.\n",
    "        :param orientation:  Is the tree top down, \"TD\", or left to right, \"LR\"?\n",
    "        :param instance_orientation: table orientation (TD, LR) for showing feature prediction's values.\n",
    "        :param leaftype: leaf plot type ('pie', 'barh')\n",
    "        :param show_root_edge_labels: Include < and >= on the edges emanating from the root?\n",
    "        :param show_node_labels: Add \"Node id\" to top of each node in graph for educational purposes\n",
    "        :param show_just_path: If True, it shows only the sample(X) prediction path\n",
    "        :param fancy:\n",
    "        :param histtype: [For classifiers] Either 'bar' or 'barstacked' to indicate\n",
    "                         histogram type. We find that 'barstacked' looks great up to about.\n",
    "                         four classes.\n",
    "        :param highlight_path: A list of node IDs to highlight, default is [].\n",
    "                               Useful for emphasizing node(s) in tree for discussion.\n",
    "                               If X argument given then this is ignored.\n",
    "        :type highlight_path: List[int]\n",
    "        :param x: Instance to run down the tree; derived path to highlight from this vector.\n",
    "                  Show feature vector with labels underneath leaf reached. highlight_path\n",
    "                  is ignored if X is not None.\n",
    "        :type x: np.ndarray\n",
    "        :param label_fontsize: Size of the label font\n",
    "        :param ticks_fontsize: Size of the tick font\n",
    "        :param fontname: Font which is used for labels and text\n",
    "        :param max_X_features_LR: If len(X) exceeds this limit for LR layout,\n",
    "                                display only those features\n",
    "                               used to guide X vector down tree. Helps when len(X) is large.\n",
    "                               Default is 10.\n",
    "        :param max_X_features_TD: If len(X) exceeds this limit for TD layout,\n",
    "                                display only those features\n",
    "                               used to guide X vector down tree. Helps when len(X) is large.\n",
    "                               Default is 25.\n",
    "        :param depth_range_to_display: range of depth levels to be displayed. The range values are inclusive\n",
    "        :param title: An optional title placed at the top of the tree.\n",
    "        :param title_fontsize: Size of the text for the title.\n",
    "        :param colors: dict A custom set of colors for visualisations\n",
    "        :param scale: Default is 1.0. Scale the width, height of the overall SVG preserving aspect ratio\n",
    "        :return: A DTreeVizRender object containing string in graphviz DOT language that\n",
    "                describes the decision tree.\n",
    "        \"\"\"\n",
    "\n",
    "        def node_name(node: ShadowDecTreeNode) -> str:\n",
    "            return f\"node{node.id}\"\n",
    "\n",
    "        def split_node(name, node_name, split):\n",
    "            if fancy:\n",
    "                filepath = os.path.join(tmp, f\"node{node.id}_{os.getpid()}.svg\")\n",
    "                labelgraph = node_label(node) if show_node_labels else ''\n",
    "                html = f\"\"\"<table border=\"0\">\n",
    "                    {labelgraph}\n",
    "                    <tr>\n",
    "                            <td><img src=\"{filepath}\"/></td>\n",
    "                    </tr>\n",
    "                    </table>\"\"\"\n",
    "            else:\n",
    "                html = f\"\"\"<font face=\"{fontname}\" color=\"{colors[\"text\"]}\" point-size=\"12\">{name}@{split}</font>\"\"\"\n",
    "            if node.id in highlight_path:\n",
    "                gr_node = f'{node_name} [margin=\"0\" shape=box penwidth=\".5\" color=\"{colors[\"highlight\"]}\" style=\"dashed\" label=<{html}>]'\n",
    "            else:\n",
    "                gr_node = f'{node_name} [margin=\"0\" shape=none label=<{html}>]'\n",
    "            return gr_node\n",
    "\n",
    "        def regr_leaf_node(node, label_fontsize: int = 12):\n",
    "            # always generate fancy regr leaves for now but shrink a bit for nonfancy.\n",
    "            labelgraph = node_label(node) if show_node_labels else ''\n",
    "            filepath = os.path.join(tmp, f\"leaf{node.id}_{os.getpid()}.svg\")\n",
    "            html = f\"\"\"<table border=\"0\">\n",
    "                {labelgraph}\n",
    "                <tr>\n",
    "                        <td><img src=\"{filepath}\"/></td>\n",
    "                </tr>\n",
    "                </table>\"\"\"\n",
    "            if node.id in highlight_path:\n",
    "                return f'leaf{node.id} [margin=\"0\" shape=box penwidth=\".5\" color=\"{colors[\"highlight\"]}\" style=\"dashed\" label=<{html}>]'\n",
    "            else:\n",
    "                return f'leaf{node.id} [margin=\"0\" shape=box penwidth=\"0\" color=\"{colors[\"text\"]}\" label=<{html}>]'\n",
    "\n",
    "        def class_leaf_node(node, label_fontsize: int = 12):\n",
    "            labelgraph = node_label(node) if show_node_labels else ''\n",
    "            filepath = os.path.join(tmp, f\"leaf{node.id}_{os.getpid()}.svg\")\n",
    "            html = f\"\"\"<table border=\"0\" CELLBORDER=\"0\">\n",
    "                {labelgraph}\n",
    "                <tr>\n",
    "                        <td><img src=\"{filepath}\"/></td>\n",
    "                        \n",
    "                        <td><img src=\"{filepath+\".svg\"}\"/></td>\n",
    "                </tr>\n",
    "                </table>\"\"\"\n",
    "            if node.id in highlight_path:\n",
    "                return f'leaf{node.id} [margin=\"0\" shape=box penwidth=\".5\" color=\"{colors[\"highlight\"]}\" style=\"dashed\" label=<{html}>]'\n",
    "            else:\n",
    "                return f'leaf{node.id} [margin=\"0\" shape=box penwidth=\"0\" color=\"{colors[\"text\"]}\" label=<{html}>]'\n",
    "\n",
    "        def node_label(node):\n",
    "            return f'<tr><td CELLPADDING=\"0\" CELLSPACING=\"0\"><font face=\"{fontname}\" color=\"{colors[\"node_label\"]}\" point-size=\"14\"><i>Node {node.id}</i></font></td></tr>'\n",
    "\n",
    "        def class_legend_html():\n",
    "            filepath = os.path.join(tmp, f\"legend_{os.getpid()}.svg\")\n",
    "            return f\"\"\"\n",
    "                <table border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n",
    "                    <tr>\n",
    "                        <td border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><img src=\"{filepath}\"/></td>\n",
    "                    </tr>\n",
    "                </table>\n",
    "                \"\"\"\n",
    "\n",
    "        def class_legend_gr():\n",
    "            if not self.shadow_tree.is_classifier():\n",
    "                return \"\"\n",
    "            return f\"\"\"\n",
    "                    subgraph cluster_legend {{\n",
    "                        style=invis;\n",
    "                        legend [penwidth=\"0\" margin=\"0\" shape=box margin=\"0.03\" width=.1, height=.1 label=<\n",
    "                        {class_legend_html()}\n",
    "                        >]\n",
    "                    }}\n",
    "                    \"\"\"\n",
    "\n",
    "        def instance_html(path, instance_fontsize: int = 11):\n",
    "            headers = []\n",
    "            features_used = [node.feature() for node in path[:-1]]  # don't include leaf\n",
    "            display_X = x\n",
    "            display_feature_names = self.shadow_tree.feature_names\n",
    "            highlight_feature_indexes = features_used\n",
    "            if (orientation == 'TD' and len(x) > max_X_features_TD) or \\\n",
    "                    (orientation == 'LR' and len(x) > max_X_features_LR):\n",
    "                # squash all features down to just those used\n",
    "                display_X = [x[i] for i in features_used] + ['...']\n",
    "                display_feature_names = [node.feature_name() for node in path[:-1]] + ['...']\n",
    "                highlight_feature_indexes = range(0, len(features_used))\n",
    "\n",
    "            for i, name in enumerate(display_feature_names):\n",
    "                if i in highlight_feature_indexes:\n",
    "                    color = colors['highlight']\n",
    "                else:\n",
    "                    color = colors['text']\n",
    "                headers.append(f'<td cellpadding=\"1\" align=\"right\" bgcolor=\"white\">'\n",
    "                               f'<font face=\"{fontname}\" color=\"{color}\" point-size=\"{instance_fontsize}\">'\n",
    "                               f'{name}'\n",
    "                               '</font>'\n",
    "                               '</td>')\n",
    "\n",
    "            values = []\n",
    "            for i, v in enumerate(display_X):\n",
    "                if i in highlight_feature_indexes:\n",
    "                    color = colors['highlight']\n",
    "                else:\n",
    "                    color = colors['text']\n",
    "                if isinstance(v, int) or isinstance(v, str):\n",
    "                    disp_v = v\n",
    "                else:\n",
    "                    disp_v = myround(v, precision)\n",
    "                values.append(f'<td cellpadding=\"1\" align=\"right\" bgcolor=\"white\">'\n",
    "                              f'<font face=\"{fontname}\" color=\"{color}\" point-size=\"{instance_fontsize}\">{disp_v}</font>'\n",
    "                              '</td>')\n",
    "\n",
    "            if instance_orientation == \"TD\":\n",
    "                html_output = \"\"\"<table border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\"\"\"\n",
    "                for header, value in zip(headers, values):\n",
    "                    html_output += f\"<tr> {header} {value} </tr>\"\n",
    "                html_output += \"</table>\"\n",
    "                return html_output\n",
    "            else:\n",
    "                return f\"\"\"\n",
    "                        <table border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n",
    "                        <tr>\n",
    "                            {''.join(headers)}\n",
    "                        </tr>\n",
    "                        <tr>\n",
    "                            {''.join(values)}\n",
    "                        </tr>\n",
    "                        </table>\n",
    "                        \"\"\"\n",
    "\n",
    "        def instance_gr():\n",
    "            if x is None:\n",
    "                return \"\"\n",
    "            path = self.shadow_tree.predict_path(x)\n",
    "            leaf = f\"leaf{path[-1].id}\"\n",
    "            if self.shadow_tree.is_classifier():\n",
    "                edge_label = f\"  Prediction<br/> {path[-1].prediction_name()}\"\n",
    "            else:\n",
    "                edge_label = f\"  Prediction<br/> {myround(path[-1].prediction(), precision)}\"\n",
    "            return f\"\"\"\n",
    "                    subgraph cluster_instance {{\n",
    "                        style=invis;\n",
    "                        X_y [penwidth=\"0.3\" margin=\"0\" shape=box margin=\"0.03\" width=.1, height=.1 label=<\n",
    "                        {instance_html(path)}\n",
    "                        >]\n",
    "                    }}\n",
    "                    {leaf} -> X_y [dir=back; penwidth=\"1.2\" color=\"{colors['highlight']}\" label=<<font face=\"{fontname}\" color=\"{colors['leaf_label']}\" point-size=\"{11}\">{edge_label}</font>>]\n",
    "                    \"\"\"\n",
    "\n",
    "        def get_internal_nodes():\n",
    "            if show_just_path and x is not None:\n",
    "                _internal = []\n",
    "                for _node in self.shadow_tree.internal:\n",
    "                    if _node.id in highlight_path:\n",
    "                        _internal.append(_node)\n",
    "                return _internal\n",
    "            else:\n",
    "                return self.shadow_tree.internal\n",
    "\n",
    "        def get_leaves():\n",
    "            if show_just_path and x is not None:\n",
    "                _leaves = []\n",
    "                for _node in self.shadow_tree.leaves:\n",
    "                    if _node.id in highlight_path:\n",
    "                        _leaves.append(_node)\n",
    "                        break\n",
    "                return _leaves\n",
    "            else:\n",
    "                return self.shadow_tree.leaves\n",
    "\n",
    "        n_classes = self.shadow_tree.nclasses()\n",
    "        colors = adjust_colors(colors, n_classes)\n",
    "\n",
    "        if orientation == \"TD\":\n",
    "            ranksep = \".2\"\n",
    "            nodesep = \"0.1\"\n",
    "        else:\n",
    "            if fancy:\n",
    "                ranksep = \".22\"\n",
    "                nodesep = \"0.1\"\n",
    "            else:\n",
    "                ranksep = \".05\"\n",
    "                nodesep = \"0.09\"\n",
    "\n",
    "        tmp = tempfile.gettempdir()\n",
    "        if x is not None:\n",
    "            path = self.shadow_tree.predict_path(x)\n",
    "            highlight_path = [n.id for n in path]\n",
    "\n",
    "        color_values = colors['classes'][n_classes]\n",
    "\n",
    "        # Fix the mapping from target value to color for entire tree\n",
    "        if self.shadow_tree.is_classifier():\n",
    "            class_values = self.shadow_tree.classes()\n",
    "            if np.max(class_values) >= n_classes:\n",
    "                raise ValueError(f\"Target label values (for now) must be 0..{n_classes-1} for n={n_classes} labels\")\n",
    "            color_map = {v: color_values[i] for i, v in enumerate(class_values)}\n",
    "            _draw_legend(self.shadow_tree, self.shadow_tree.target_name, os.path.join(tmp, f\"legend_{os.getpid()}.svg\"),\n",
    "                         colors=colors,\n",
    "                         fontname=fontname)\n",
    "\n",
    "        X_train = self.shadow_tree.X_train\n",
    "        y_train = self.shadow_tree.y_train\n",
    "        if isinstance(X_train, pd.DataFrame):\n",
    "            X_train = X_train.values\n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y_train = y_train.values\n",
    "        if y_train.dtype == np.dtype(object):\n",
    "            try:\n",
    "                y_train = y_train.astype('float')\n",
    "            except ValueError as e:\n",
    "                raise ValueError('y_train needs to consist only of numerical values. {}'.format(e))\n",
    "            if len(y_train.shape) != 1:\n",
    "                raise ValueError('y_train must a one-dimensional list or Pandas Series, got: {}'.format(y_train.shape))\n",
    "\n",
    "        y_range = (min(y_train) * 1.03, max(y_train) * 1.03)  # same y axis for all\n",
    "\n",
    "        # Find max height (count) for any bar in any node\n",
    "        if self.shadow_tree.is_classifier():\n",
    "            nbins = _get_num_bins(histtype, n_classes)\n",
    "            node_heights = self.shadow_tree.get_split_node_heights(X_train, y_train, nbins=nbins)\n",
    "\n",
    "        internal = []\n",
    "        for node in get_internal_nodes():\n",
    "            if depth_range_to_display is not None:\n",
    "                if node.level not in range(depth_range_to_display[0], depth_range_to_display[1] + 1):\n",
    "                    continue\n",
    "            if fancy:\n",
    "                if self.shadow_tree.is_classifier():\n",
    "                    _class_split_viz(node, X_train, y_train,\n",
    "                                     filename=os.path.join(tmp, f\"node{node.id}_{os.getpid()}.svg\"),\n",
    "                                     precision=precision,\n",
    "                                     colors={**color_map, **colors},\n",
    "                                     histtype=histtype,\n",
    "                                     node_heights=node_heights,\n",
    "                                     X=x,\n",
    "                                     ticks_fontsize=ticks_fontsize,\n",
    "                                     label_fontsize=label_fontsize,\n",
    "                                     fontname=fontname,\n",
    "                                     highlight_node=node.id in highlight_path)\n",
    "                else:\n",
    "                    _regr_split_viz(node, X_train, y_train,\n",
    "                                    filename=os.path.join(tmp, f\"node{node.id}_{os.getpid()}.svg\"),\n",
    "                                    target_name=self.shadow_tree.target_name,\n",
    "                                    y_range=y_range,\n",
    "                                    X=x,\n",
    "                                    ticks_fontsize=ticks_fontsize,\n",
    "                                    label_fontsize=label_fontsize,\n",
    "                                    fontname=fontname,\n",
    "                                    highlight_node=node.id in highlight_path,\n",
    "                                    colors=colors)\n",
    "\n",
    "            nname = node_name(node)\n",
    "            if not node.is_categorical_split():\n",
    "                gr_node = split_node(node.feature_name(), nname, split=myround(node.split(), precision))\n",
    "            else:\n",
    "                gr_node = split_node(node.feature_name(), nname, split=node.split()[0])\n",
    "            internal.append(gr_node)\n",
    "\n",
    "        leaves = []\n",
    "        for node in get_leaves():\n",
    "            if depth_range_to_display is not None:\n",
    "                if node.level not in range(depth_range_to_display[0], depth_range_to_display[1] + 1):\n",
    "                    continue\n",
    "            if self.shadow_tree.is_classifier():\n",
    "                _class_leaf_viz(node, colors=color_values,\n",
    "                                filename=os.path.join(tmp, f\"leaf{node.id}_{os.getpid()}.svg\"),\n",
    "                                graph_colors=colors,\n",
    "                                fontname=fontname,\n",
    "                                leaftype=leaftype)\n",
    "                leaves.append(class_leaf_node(node))\n",
    "            else:\n",
    "                # for now, always gen leaf\n",
    "                _regr_leaf_viz(node,\n",
    "                               y_train,\n",
    "                               target_name=self.shadow_tree.target_name,\n",
    "                               filename=os.path.join(tmp, f\"leaf{node.id}_{os.getpid()}.svg\"),\n",
    "                               y_range=y_range,\n",
    "                               precision=precision,\n",
    "                               ticks_fontsize=ticks_fontsize,\n",
    "                               label_fontsize=label_fontsize,\n",
    "                               fontname=fontname,\n",
    "                               colors=colors)\n",
    "                leaves.append(regr_leaf_node(node))\n",
    "\n",
    "        if show_just_path:\n",
    "            show_root_edge_labels = False\n",
    "\n",
    "        # TODO do we need show_edge_labels ?\n",
    "        show_edge_labels = False\n",
    "        all_llabel = '  &lt;' if show_edge_labels else ''\n",
    "        all_rlabel = '  &ge;' if show_edge_labels else ''\n",
    "        root_llabel = f'  {self.shadow_tree.get_root_edge_labels()[0]}' if show_root_edge_labels else ''\n",
    "        root_rlabel = f'  {self.shadow_tree.get_root_edge_labels()[1]}' if show_root_edge_labels else ''\n",
    "\n",
    "        edges = []\n",
    "        # non leaf edges with > and <=\n",
    "        for node in get_internal_nodes():\n",
    "            if depth_range_to_display is not None:\n",
    "                if node.level not in range(depth_range_to_display[0], depth_range_to_display[1]):\n",
    "                    continue\n",
    "            nname = node_name(node)\n",
    "            if node.left.isleaf():\n",
    "                left_node_name = 'leaf%d' % node.left.id\n",
    "            else:\n",
    "                left_node_name = node_name(node.left)\n",
    "            if node.right.isleaf():\n",
    "                right_node_name = 'leaf%d' % node.right.id\n",
    "            else:\n",
    "                right_node_name = node_name(node.right)\n",
    "\n",
    "            if node == self.shadow_tree.root:\n",
    "                llabel = root_llabel\n",
    "                rlabel = root_rlabel\n",
    "            else:\n",
    "                llabel = all_llabel\n",
    "                rlabel = all_rlabel\n",
    "\n",
    "            if node.is_categorical_split() and not self.shadow_tree.is_classifier():\n",
    "                lcolor, rcolor = colors[\"categorical_split_left\"], colors[\"categorical_split_right\"]\n",
    "            else:\n",
    "                lcolor = colors.get('larrow', colors['arrow'])\n",
    "                rcolor = colors.get('rarrow', colors['arrow'])\n",
    "\n",
    "            lpw = rpw = \"0.3\"\n",
    "            if node.left.id in highlight_path:\n",
    "                lcolor = colors['highlight']\n",
    "                lpw = \"1.2\"\n",
    "            if node.right.id in highlight_path:\n",
    "                rcolor = colors['highlight']\n",
    "                rpw = \"1.2\"\n",
    "\n",
    "            if show_just_path:\n",
    "                if node.left.id in highlight_path:\n",
    "                    edges.append(f'{nname} -> {left_node_name} [penwidth={lpw} fontname=\"{fontname}\" color=\"{lcolor}\" label=<{llabel}> fontcolor=\"{colors[\"text\"]}\"]')\n",
    "                if node.right.id in highlight_path:\n",
    "                    edges.append(f'{nname} -> {right_node_name} [penwidth={rpw} fontname=\"{fontname}\" color=\"{rcolor}\" label=<{rlabel}> fontcolor=\"{colors[\"text\"]}\"]')\n",
    "            else:\n",
    "                edges.append(f'{nname} -> {left_node_name} [penwidth={lpw} fontname=\"{fontname}\" color=\"{lcolor}\" label=<{llabel}> fontcolor=\"{colors[\"text\"]}\"]')\n",
    "                edges.append(f'{nname} -> {right_node_name} [penwidth={rpw} fontname=\"{fontname}\" color=\"{rcolor}\" label=<{rlabel}> fontcolor=\"{colors[\"text\"]}\"]')\n",
    "                edges.append(f\"\"\"\n",
    "                    {{\n",
    "                        rank=same;\n",
    "                        {left_node_name} -> {right_node_name} [style=invis]\n",
    "                    }}\n",
    "                    \"\"\")\n",
    "\n",
    "        newline = \"\\n\\t\"\n",
    "        if title:\n",
    "            title_element = f'graph [label=\"{title}\", labelloc=t, fontname=\"{fontname}\" fontsize={title_fontsize} fontcolor=\"{colors[\"title\"]}\"];'\n",
    "        else:\n",
    "            title_element = \"\"\n",
    "        dot = f\"\"\"\n",
    "        digraph G {{\n",
    "            splines=line;\n",
    "            nodesep={nodesep};\n",
    "            ranksep={ranksep};\n",
    "            rankdir={orientation};\n",
    "            margin=0.0;\n",
    "            {title_element}\n",
    "            node [margin=\"0.03\" penwidth=\"0.5\" width=.1, height=.1];\n",
    "            edge [arrowsize=.4 penwidth=\"0.3\"]\n",
    "\n",
    "            {newline.join(internal)}\n",
    "            {newline.join(edges)}\n",
    "            {newline.join(leaves)}\n",
    "\n",
    "            {class_legend_gr()}\n",
    "            {instance_gr()}\n",
    "        }}\n",
    "            \"\"\"\n",
    "\n",
    "        return DTreeVizRender(dot, scale)\n",
    "\n",
    "    def leaf_purity(self,\n",
    "                    display_type: str = \"plot\",\n",
    "                    colors: dict = None,\n",
    "                    fontsize: int = 10,\n",
    "                    fontname: str = \"Arial\",\n",
    "                    grid: bool = False,\n",
    "                    bins: int = 10,\n",
    "                    figsize: tuple = None,\n",
    "                    ax=None):\n",
    "        \"\"\"Visualize leaves criterion/purities.\n",
    "\n",
    "        The most common criterion/purity for tree regressors is â€œmseâ€, â€œfriedman_mseâ€, â€œmaeâ€ and for tree classifers are\n",
    "        \"gini\" and \"entropy\". This information shows the leaf performance/confidence for its predictions, namely how pure or\n",
    "        impure are the samples from each leaf. Each leaf performance, in the end, will determine the general tree performance.\n",
    "\n",
    "        This visualisation can be used together with leaf_sizes() for a better leaf interpretation. For example,\n",
    "        a leaf with good confidence, but few samples, can be a sign of overfitting. The best scenario would be to have a\n",
    "        leaf with good confidence and also a lot of samples.\n",
    "\n",
    "\n",
    "        Usage example :\n",
    "        viz_model = dtreeviz.model(tree_model, X_train=dataset[features], y_train=dataset[target],\n",
    "                                   feature_names=features, target_name=target, class_names=[0, 1])\n",
    "        viz_model.leaf_purity()\n",
    "\n",
    "        This method contains three types of visualizations:\n",
    "        - a plot bar visualisations for each leaf criterion, when we want to interpret individual leaves\n",
    "        - a hist visualizations with leaf criterion, when we want to have a general overview for all leaves\n",
    "        - a text visualisations, useful when number of leaves is very large and visual interpretation becomes difficult.\n",
    "\n",
    "        :param display_type: str, optional\n",
    "           'plot', 'text'. 'hist'\n",
    "        :param colors: dict\n",
    "            A custom set of colors for visualisations\n",
    "        :param fontsize: int\n",
    "            Plot labels font size\n",
    "        :param fontname: str\n",
    "            Plot labels font name\n",
    "        :param grid: bool\n",
    "            True if we want to display the grid lines on the visualization\n",
    "        :param bins:  int\n",
    "            Number of histogram bins\n",
    "        :param figsize: optional (width, height) in inches for the entire plot\n",
    "            :param ax: optional matplotlib \"axes\" to draw into\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        leaf_id, leaf_criteria = self.shadow_tree.get_leaf_criterion()\n",
    "\n",
    "        if display_type == \"text\":\n",
    "            for leaf, criteria in zip(leaf_id, leaf_criteria):\n",
    "                print(f\"leaf {leaf} has {criteria} {self.shadow_tree.criterion()}\")\n",
    "        elif display_type in [\"plot\", \"hist\"]:\n",
    "            colors = adjust_colors(colors)\n",
    "            if ax is None:\n",
    "                if figsize:\n",
    "                    fig, ax = plt.subplots(figsize=figsize)\n",
    "                else:\n",
    "                    fig, ax = plt.subplots()\n",
    "        else:\n",
    "            raise ValueError(f'Unknown display_type = {display_type}')\n",
    "\n",
    "        if display_type == \"plot\":\n",
    "            ax.set_xticks(range(0, len(leaf_id)))\n",
    "            ax.set_xticklabels(leaf_id)\n",
    "            barcontainers = ax.bar(range(0, len(leaf_id)), leaf_criteria, color=colors[\"hist_bar\"], lw=.3,\n",
    "                                   align='center',\n",
    "                                   width=1)\n",
    "            for rect in barcontainers.patches:\n",
    "                rect.set_linewidth(.5)\n",
    "                rect.set_edgecolor(colors['rect_edge'])\n",
    "\n",
    "            _format_axes(ax, \"Leaf IDs\", self.shadow_tree.criterion(), colors, fontsize, fontname, ticks_fontsize=None, grid=grid)\n",
    "\n",
    "        elif display_type == \"hist\":\n",
    "            n, bins, patches = ax.hist(leaf_criteria, bins=bins, color=colors[\"hist_bar\"])\n",
    "            for rect in patches:\n",
    "                rect.set_linewidth(.5)\n",
    "                rect.set_edgecolor(colors['rect_edge'])\n",
    "\n",
    "            _format_axes(ax, self.shadow_tree.criterion(), \"Leaf Count\", colors, fontsize, fontname, ticks_fontsize=None, grid=grid)\n",
    "\n",
    "    def node_stats(self, node_id: int) -> pd.DataFrame:\n",
    "        \"\"\"Generate stats (count, mean, std, etc) based on data samples from a specified node.\n",
    "\n",
    "        This method is especially useful to investigate leaf samples from a decision tree. This is a way to discover data\n",
    "        patterns, to better understand our tree model and to get new ideas for feature generation.\n",
    "\n",
    "\n",
    "        Usage example :\n",
    "        viz_model = dtreeviz.model(tree_model, X_train=dataset[features], y_train=dataset[target],\n",
    "                                   feature_names=features, target_name=target, class_names=[0, 1])\n",
    "        viz_model.node_stats(node_id=10)\n",
    "\n",
    "        :param node_id: int\n",
    "            Node id to interpret\n",
    "        :return: pd.DataFrame\n",
    "            Node training samples' stats\n",
    "        \"\"\"\n",
    "\n",
    "        node_samples = self.shadow_tree.get_node_samples()\n",
    "        df = pd.DataFrame(self.shadow_tree.X_train, columns=self.shadow_tree.feature_names).convert_dtypes()\n",
    "        return df.iloc[node_samples[node_id]].describe(include='all')\n",
    "\n",
    "    def instance_feature_importance(self, x,\n",
    "                                   colors: dict = None,\n",
    "                                   fontsize: int = 10,\n",
    "                                   fontname: str = \"Arial\",\n",
    "                                   grid: bool = False,\n",
    "                                   figsize: tuple = None,\n",
    "                                   ax=None):\n",
    "        \"\"\"Prediction feature importance for a data instance.\n",
    "\n",
    "        There will be created a visualisation for feature importance, just like the popular one from sklearn library,\n",
    "        but in this scencario, the feature importances will be calculated based only on the nodes from prediction path.\n",
    "\n",
    "        Usage example :\n",
    "        viz_model = dtreeviz.model(tree_model, X_train=dataset[features], y_train=dataset[target],\n",
    "                                   feature_names=features, target_name=target, class_names=[0, 1])\n",
    "        viz_model.instance_feature_importance(x)\n",
    "\n",
    "        :param x: Instance example to make prediction\n",
    "        :param colors: dict, optional\n",
    "            The set of colors used for plotting\n",
    "        :param fontsize: int, optional\n",
    "            Plot labels fontsize\n",
    "        :param fontname: str, optional\n",
    "            Plot labels font name\n",
    "        :param grid: bool\n",
    "            True if we want to display the grid lines on the visualization\n",
    "        :param figsize: optional (width, height) in inches for the entire plot\n",
    "            :param ax: optional matplotlib \"axes\" to draw into\n",
    "        \"\"\"\n",
    "        explain_prediction_sklearn_default(self.shadow_tree, x,\n",
    "                                           colors,\n",
    "                                           fontsize,\n",
    "                                           fontname,\n",
    "                                           grid,\n",
    "                                           figsize,\n",
    "                                           ax)\n",
    "\n",
    "    def explain_prediction_path(self, x: np.ndarray) -> str:\n",
    "        \"\"\"Prediction path interpretation for a data instance. There will be created a range of values for each feature,\n",
    "         based on data instance values and its tree prediction path.\n",
    "\n",
    "        A possible output for this method could be :\n",
    "            1.5 <= Pclass\n",
    "            3.5 <= Age < 44.5\n",
    "            7.91 <= Fare < 54.25\n",
    "            0.5 <= Sex_label\n",
    "            Cabin_label < 3.5\n",
    "            0.5 <= Embarked_label\n",
    "\n",
    "        Usage example :\n",
    "        viz_model = dtreeviz.model(tree_model, X_train=dataset[features], y_train=dataset[target],\n",
    "                                   feature_names=features, target_name=target, class_names=[0, 1])\n",
    "        viz_model.explain_prediction_path(x)\n",
    "\n",
    "        :param x: np.ndarray\n",
    "            The data instance for which we want to investigate prediction path\n",
    "        \"\"\"\n",
    "        return explain_prediction_plain_english(self.shadow_tree, x)\n",
    "\n",
    "    def rtree_leaf_distributions(self,\n",
    "                                 show_leaf_labels: bool = True,\n",
    "                                 colors: dict = None,\n",
    "                                 markersize: int = 50,\n",
    "                                 label_fontsize: int = 10,\n",
    "                                 fontname: str = \"Arial\",\n",
    "                                 precision: int = 1,\n",
    "                                 grid: bool = False,\n",
    "                                 prediction_line_width: int = 2,\n",
    "                                 figsize: tuple = None,\n",
    "                                 ax=None):\n",
    "        \"\"\"Visualize leaf target distributions for regression decision trees.\n",
    "\n",
    "        Usage example :\n",
    "        viz_model = dtreeviz.model(tree_model, X_train=dataset[features], y_train=dataset[target],\n",
    "                                   feature_names=features, target_name=target, class_names=[0, 1])\n",
    "        viz_model.rtree_leaf_distributions()\n",
    "\n",
    "        :param show_leaf_labels: bool\n",
    "            True if the plot should contains the leaf labels on x ax, False otherwise.\n",
    "        :param colors: dict\n",
    "            A custom set of colors for visualisations\n",
    "        :param markersize: int\n",
    "            Marker size in points.\n",
    "        :param label_fontsize: int\n",
    "            Size of the label font\n",
    "        :param fontname: str\n",
    "            Font which is used for labels and text\n",
    "        :param precision: int\n",
    "            When displaying floating-point numbers, how many digits to display after the decimal point. Default is 1.\n",
    "        :param grid: bool\n",
    "            True if we want to display the grid lines on the visualization\n",
    "        :param prediction_line_width: int\n",
    "            The width of prediction line.\n",
    "        :param figsize: optional (width, height) in inches for the entire plot\n",
    "            :param ax: optional matplotlib \"axes\" to draw into\n",
    "        \"\"\"\n",
    "        x, y, means, means_range, y_labels = _get_leaf_target_input(self.shadow_tree, precision)\n",
    "        colors = adjust_colors(colors)\n",
    "        if ax is None:\n",
    "            if figsize is None:\n",
    "                figsize = (np.log(len(y_labels)), np.log(len(y_labels)) * 1.5) if figsize is None else figsize\n",
    "            fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "        ax.set_xlim(min(y), max(y) + 10)\n",
    "        ax.set_ylim(-1, len(y_labels))\n",
    "        ax.set_yticks(np.arange(0, len(y_labels), 1))\n",
    "        ax.set_yticklabels([])\n",
    "        ax.scatter(y, x, marker='o', alpha=colors['scatter_marker_alpha'] - 0.2, c=colors['scatter_marker'],\n",
    "                   s=markersize,\n",
    "                   edgecolor=colors['scatter_edge'], lw=.3)\n",
    "\n",
    "        if show_leaf_labels:\n",
    "            for i in range(len(y_labels)):\n",
    "                ax.text(max(y) + 10, i - 0.15, y_labels[i])\n",
    "            ax.text(max(y) + 10, len(y_labels) - 0.15, self.shadow_tree.target_name.lower())\n",
    "\n",
    "        for i in range(len(means)):\n",
    "            ax.plot(means[i], means_range[i], color=colors['split_line'], linewidth=prediction_line_width)\n",
    "\n",
    "        _format_axes(ax, self.shadow_tree.target_name, \"Leaf IDs\", colors, fontsize=label_fontsize, fontname=fontname, ticks_fontsize=None, grid=grid)\n",
    "\n",
    "    def ctree_feature_space(self,\n",
    "                            fontsize=10,\n",
    "                            ticks_fontsize=8,\n",
    "                            fontname=\"Arial\",\n",
    "                            nbins=25,\n",
    "                            gtype='strip',\n",
    "                            show={'title', 'legend', 'splits'},\n",
    "                            colors=None,\n",
    "                            features=None,\n",
    "                            figsize=None,\n",
    "                            ax=None):\n",
    "        \"\"\"\n",
    "        Decision trees partition feature space into rectangular regions\n",
    "        through the series of splits (at internal decision nodes) along the\n",
    "        path from the root to a leaf while making a prediction for an input\n",
    "        instance. The complete tessellation of feature space is the collection\n",
    "        of regions inscribed by all paths from the root to a leaf.\n",
    "\n",
    "        This function isolates one or two features of interest according to\n",
    "        the features parameter and generates a plot.  For one feature, the\n",
    "        resulting plot has that feature on the X axis and the associated class\n",
    "        targets at different elevations (with some noise) on the Y axis to\n",
    "        separate them.  (Use gtype='barstacked' to get a histogram instead.)\n",
    "        For two features, the plot has the two features of\n",
    "        interest on the X and Y axes and plots the 2D coordinate for each\n",
    "        training data instance. Each marker and region has a unique color\n",
    "        according to the classification label.\n",
    "\n",
    "        Decision nodes associated with features not in the features parameter\n",
    "        do not contribute to the tessellation of the feature space. Paths from\n",
    "        the root to leaves in the decision tree do not contribute a region unless\n",
    "        one or more of the features of interest is tested.  Given a model\n",
    "        trained with exactly two features, calling this function with those\n",
    "        two features results in disjoint regions. Any coordinate in 2D feature\n",
    "        space would always lead to a unique leaf.\n",
    "\n",
    "        When the model is trained on more than two features, however, the same\n",
    "        coordinate in the 2D feature space of interest could appear in\n",
    "        multiple paths from the root to a leaf. Consequently, it is possible\n",
    "        for regions to overlap because the tree is testing other variables\n",
    "        along those paths (so that each coordinate in d-space for d model\n",
    "        features still reaches a unique leaf).  Overlapping regions simply\n",
    "        means another feature would disambiguate those regions during model\n",
    "        inference.\n",
    "\n",
    "        :param gtype: {'strip','barstacked'}\n",
    "        :param show: Plot elements to show: {'title', 'legend', 'splits'}\n",
    "        :param features: A list of strings containing one or two features of interest.\n",
    "                         If none is specified, the first feature(s) in X_training dataframe are used.\n",
    "        :param figsize: Width and height in inches for the figure; use something like (5,1)\n",
    "                        for len(features)==1 and (5,3) for len(features)==1.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: check if we can find some common functionality between univar and bivar visualisations and refactor\n",
    "        #  to a single method.\n",
    "        if features is None:\n",
    "            n_features = len(self.shadow_tree.feature_names)\n",
    "            features = self.shadow_tree.feature_names[0:min(n_features,2)] # pick first one/two features if none given\n",
    "        if len(features) == 1:     # univar example\n",
    "            _ctreeviz_univar(self.shadow_tree, fontsize, ticks_fontsize, fontname, nbins, gtype, show, colors, features[0], figsize, ax)\n",
    "        elif len(features) == 2:   # bivar example\n",
    "            _ctreeviz_bivar(self.shadow_tree, fontsize, ticks_fontsize, fontname, show, colors, features, figsize, ax)\n",
    "        else:\n",
    "            raise ValueError(f\"ctree_feature_space supports a dataset with only one or two features.\"\n",
    "                             f\" You provided a dataset with {len(self.shadow_tree.feature_names)} features {self.shadow_tree.feature_names}.\")\n",
    "\n",
    "    def rtree_feature_space(self, fontsize: int = 10, ticks_fontsize=8, show={'title', 'splits'}, split_linewidth=.5,\n",
    "                            mean_linewidth=2, markersize=15, colors=None, fontname=\"Arial\",\n",
    "                            n_colors_in_map=100, features=None,\n",
    "                            figsize=None, ax=None):\n",
    "        \"\"\"\n",
    "        Decision trees partition feature space into rectangular regions\n",
    "        through the series of splits (at internal decision nodes) along the\n",
    "        path from the root to a leaf while making a prediction for an input\n",
    "        instance. The complete tessellation of feature space is the collection\n",
    "        of regions inscribed by all paths from the root to a leaf.\n",
    "\n",
    "        This function isolates one or two features of interest according to\n",
    "        the features parameter and generates a plot.  For one feature, the\n",
    "        resulting plot has that feature on the X axis and the regression\n",
    "        target on the Y axis. For two features, the plot has the two features\n",
    "        of interest on the X and Y axes and a colored heat map to indicate the\n",
    "        regression target. The darker the color, the larger the regression\n",
    "        value.\n",
    "\n",
    "        Decision nodes associated with features not in the features parameter\n",
    "        do not contribute to the tessellation of the feature space. Paths from\n",
    "        the root to leaves in the decision tree do not contribute a region unless\n",
    "        one or more of the features of interest is tested.  Given a model\n",
    "        trained with exactly two features, calling this function with those\n",
    "        two features results in disjoint regions. Any coordinate in 2D feature\n",
    "        space would always lead to a unique leaf.\n",
    "\n",
    "        When the model is trained on more than two features, however, the same\n",
    "        coordinate in the 2D feature space of interest could appear in\n",
    "        multiple paths from the root to a leaf. Consequently, it is possible\n",
    "        for regions to overlap because the tree is testing other variables\n",
    "        along those paths (so that each coordinate in d-space for d model\n",
    "        features still reaches a unique leaf).  Overlapping regions simply\n",
    "        means another feature would disambiguate those regions during model\n",
    "        inference.\n",
    "\n",
    "        :param show: which or all of {'title', 'splits'} to show\n",
    "        :param features: A list of strings containing one or two features of interest.\n",
    "                         If none is specified, the first feature(s) in X_training dataframe are used.\n",
    "        :param figsize: Width and height in inches for the figure; use something like (5,1)\n",
    "                        for len(features)==1 and (5,3) for len(features)==1.\n",
    "        \"\"\"\n",
    "        if features is None:\n",
    "            n_features = len(self.shadow_tree.feature_names)\n",
    "            features = self.shadow_tree.feature_names[0:min(n_features,2)] # pick first one/two features if none given\n",
    "        if len(features) == 1:  # univar example\n",
    "            _rtreeviz_univar(self.shadow_tree, fontsize, ticks_fontsize, fontname, show, split_linewidth, mean_linewidth, markersize, colors,\n",
    "                             features[0], figsize, ax)\n",
    "        elif len(features) == 2:  # bivar example\n",
    "            _rtreeviz_bivar_heatmap(self.shadow_tree, fontsize, ticks_fontsize, fontname, show, n_colors_in_map, colors,\n",
    "                                    markersize, features, figsize, ax)\n",
    "        else:\n",
    "            raise ValueError(f\"rtree_feature_space() supports a dataset with only one or two features.\"\n",
    "                             f\" You provided a dataset with {len(self.shadow_tree.feature_names)} features {self.shadow_tree.feature_names}.\")\n",
    "\n",
    "    def rtree_feature_space3D(self,\n",
    "                              fontsize=10, ticks_fontsize=8, fontname=\"Arial\",\n",
    "                              azim=0, elev=0, dist=7,\n",
    "                              show={'title'}, colors=None, markersize=15,\n",
    "                              n_colors_in_map=100,\n",
    "                              features=None,\n",
    "                              figsize=None, ax=None):\n",
    "        \"\"\"\n",
    "        Decision trees partition feature space into rectangular regions\n",
    "        through the series of splits (at internal decision nodes) along the\n",
    "        path from the root to a leaf while making a prediction for an input\n",
    "        instance. The complete tessellation of feature space is the collection\n",
    "        of regions inscribed by all paths from the root to a leaf.\n",
    "\n",
    "        This function isolates two features of interest according to\n",
    "        the features parameter and generates a 3D plot.  The plot has the two features\n",
    "        of interest on the X and Y axes and the regression target on the Z axis.\n",
    "        The darker the region color, the larger the regression value.\n",
    "\n",
    "        Decision nodes associated with features not in the features parameter\n",
    "        do not contribute to the tessellation of the feature space. Paths from\n",
    "        the root to leaves in the decision tree do not contribute a region unless\n",
    "        one or more of the features of interest is tested.  Given a model\n",
    "        trained with exactly two features, calling this function with those\n",
    "        two features results in disjoint regions. Any coordinate in 2D feature\n",
    "        space would always lead to a unique leaf.\n",
    "\n",
    "        When the model is trained on more than two features, however, the same\n",
    "        coordinate in the 2D feature space of interest could appear in\n",
    "        multiple paths from the root to a leaf. Consequently, it is possible\n",
    "        for regions to overlap vertically in the 3D plot because the tree is\n",
    "        testing other variables along those paths (so that each coordinate\n",
    "        in d-space for d model features still reaches a unique leaf).\n",
    "        Overlapping regions simply means another feature would disambiguate\n",
    "        those regions during model inference.\n",
    "\n",
    "        :param azim: Angle rotation around the z axis (default 0)\n",
    "        :param elev: Elevation in degrees above the x-y axis (2D feature space) (default 0)\n",
    "        :param dist: Distance of the camera to the plot (default is 7)\n",
    "        :param show: which or all of {'title', 'splits'} to show\n",
    "        :param features: A list of strings containing one or two features of interest.\n",
    "                         If none is specified, the first feature(s) in X_training dataframe are used.\n",
    "        :param figsize: Width and height in inchesFor the figure; use something like (5,1)\n",
    "                        for len(features)==1 and (5,3) for len(features)==1.\n",
    "        \"\"\"\n",
    "        if features is None:\n",
    "            n_features = len(self.shadow_tree.feature_names)\n",
    "            if n_features >= 2:\n",
    "                features = self.shadow_tree.feature_names[0:2] # pick first two features if none given\n",
    "            else:\n",
    "                raise ValueError(f\"rtree_feature_space3D() requires at least two features; but the model has {n_features}\")\n",
    "        elif len(features)!=2:\n",
    "            raise ValueError(f\"rtree_feature_space3D() requires exactly two features; found {len(features)}\")\n",
    "\n",
    "        _rtreeviz_bivar_3D(self.shadow_tree, fontsize, ticks_fontsize, fontname,\n",
    "                           azim, elev, dist,\n",
    "                           show, colors, markersize,\n",
    "                           n_colors_in_map,\n",
    "                           features,\n",
    "                           figsize, ax)\n",
    "\n",
    "\n",
    "def _class_split_viz(node: ShadowDecTreeNode,\n",
    "                     X_train: np.ndarray,\n",
    "                     y_train: np.ndarray,\n",
    "                     colors: dict,\n",
    "                     node_heights,\n",
    "                     filename: str,\n",
    "                     ticks_fontsize: int,\n",
    "                     label_fontsize: int,\n",
    "                     fontname: str,\n",
    "                     precision: int,\n",
    "                     histtype: ('bar', 'barstacked', 'strip'),\n",
    "                     X: np.array,\n",
    "                     highlight_node: bool):\n",
    "    height_range = (.5, 1.5)\n",
    "    h = _prop_size(n=node_heights[node.id], counts=node_heights.values(), output_range=height_range)\n",
    "    figsize = (3.3, h)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    feature_name = node.feature_name()\n",
    "\n",
    "    # Get X, y data for all samples associated with this node.\n",
    "    X_feature = X_train[:, node.feature()]\n",
    "    X_node_feature, y_train = X_feature[node.samples()], y_train[node.samples()]\n",
    "\n",
    "    n_classes = node.shadow_tree.nclasses()\n",
    "    nbins = _get_num_bins(histtype, n_classes)\n",
    "\n",
    "    # for categorical splits, we need to ensure that each vertical bar will represent only one categorical feature value\n",
    "    if node.is_categorical_split():\n",
    "        feature_unique_size = np.unique(X_feature).size\n",
    "        # keep the bar widths as uniform as possible for all node visualisations\n",
    "        nbins = nbins if nbins > feature_unique_size else feature_unique_size + 1\n",
    "\n",
    "    # only for str categorical features which are str type, int categorical features can work fine as numerical ones\n",
    "    if node.is_categorical_split() and type(X_feature[0]) == str:\n",
    "        # TODO think if the len() should be from all training[feature] data vs only data from this specific node ?\n",
    "        overall_feature_range = (0, len(np.unique(X_feature)) - 1)\n",
    "    else:\n",
    "        overall_feature_range = (np.min(X_train[:, node.feature()]), np.max(X_train[:, node.feature()]))\n",
    "\n",
    "    bins = np.linspace(start=overall_feature_range[0], stop=overall_feature_range[1], num=nbins, endpoint=True)\n",
    "    _format_axes(ax, feature_name, None, colors, fontsize=label_fontsize, fontname=fontname, ticks_fontsize=ticks_fontsize, grid=False, pad_for_wedge=True)\n",
    "\n",
    "    class_names = node.shadow_tree.class_names\n",
    "    class_values = node.shadow_tree.classes()\n",
    "    X_hist = [X_node_feature[y_train == cl] for cl in class_values]\n",
    "\n",
    "    # for multiclass examples, there could be scenarios where a node won't contain all the class value labels which will\n",
    "    # generate a matplotlib exception. To solve this, we need to filter only the class values which belong to a node and\n",
    "    # theirs corresponding colors.\n",
    "    X_colors = [colors[cl] for i, cl in enumerate(class_values) if len(X_hist[i]) > 0]\n",
    "    X_hist = [hist for hist in X_hist if len(hist) > 0]\n",
    "\n",
    "    if histtype == 'strip':\n",
    "        ax.yaxis.set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        sigma = .013\n",
    "        mu = .05\n",
    "        class_step = .08\n",
    "        dot_w = 20\n",
    "        ax.set_ylim(0, mu + n_classes * class_step)\n",
    "        for i, bucket in enumerate(X_hist):\n",
    "            alpha = colors['scatter_marker_alpha'] if len(bucket) > 10 else 1\n",
    "            y_noise = np.random.normal(mu + i * class_step, sigma, size=len(bucket))\n",
    "            ax.scatter(bucket, y_noise, alpha=alpha, marker='o', s=dot_w, c=colors[i],\n",
    "                       edgecolors=colors['edge'], lw=.3)\n",
    "    else:\n",
    "        hist, bins, barcontainers = ax.hist(X_hist,\n",
    "                                            color=X_colors,\n",
    "                                            align='mid',\n",
    "                                            histtype=histtype,\n",
    "                                            bins=bins,\n",
    "                                            label=class_names)\n",
    "\n",
    "        # Alter appearance of each bar\n",
    "        if isinstance(barcontainers[0], matplotlib.container.BarContainer):\n",
    "            for patch in barcontainers:\n",
    "                for rect in patch.patches:\n",
    "                    rect.set_linewidth(.5)\n",
    "                    rect.set_edgecolor(colors['rect_edge'])\n",
    "            ax.set_yticks([0, max([max(h) for h in hist])])\n",
    "        elif isinstance(barcontainers[0], matplotlib.patches.Rectangle):\n",
    "            # In case a node will contains samples from only one class.\n",
    "            for rect in barcontainers.patches:\n",
    "                rect.set_linewidth(.5)\n",
    "                rect.set_edgecolor(colors['rect_edge'])\n",
    "            ax.set_yticks([0, max(hist)])\n",
    "        ####\n",
    "\n",
    "\n",
    "\n",
    "    # set an empty space at the beginning and the end of the node visualisation for better clarity\n",
    "    bin_length = bins[1] - bins[0]\n",
    "    overall_feature_range_wide = (bins[0] - 2 * bin_length, bins[len(bins) - 1] + 2 * bin_length)\n",
    "\n",
    "    ax.set_xlim(*overall_feature_range_wide)\n",
    "\n",
    "    if node.is_categorical_split() and type(X_feature[0]) == str:\n",
    "        # run draw to refresh the figure to get the xticklabels\n",
    "        plt.draw()\n",
    "        node_split = list(map(str, node.split()))\n",
    "        # get the label text and its position from the figure\n",
    "        label_index = dict([(label.get_text(), label.get_position()[0]) for label in ax.get_xticklabels()])\n",
    "        # get tick positions, ignoring \"out of dictionary\" symbol added by tensorflow trees for \"unknown symbol\"\n",
    "        wedge_ticks_position = [label_index[split] for split in node_split if split in label_index]\n",
    "        wedge_ticks = _draw_wedge(ax, x=wedge_ticks_position, node=node, color=colors['wedge'], is_classifier=True, h=h,\n",
    "                                  height_range=height_range, bins=bins)\n",
    "        if highlight_node:\n",
    "            highlight_value = [label_index[X[node.feature()]]]\n",
    "            _ = _draw_wedge(ax, x=highlight_value, node=node, color=colors['highlight'], is_classifier=True, h=h,\n",
    "                            height_range=height_range, bins=bins)\n",
    "    else:\n",
    "        wedge_ticks = _draw_wedge(ax, x=node.split(), node=node, color=colors['wedge'], is_classifier=True, h=h, height_range=height_range, bins=bins)\n",
    "        if highlight_node:\n",
    "            _ = _draw_wedge(ax, x=X[node.feature()], node=node, color=colors['highlight'], is_classifier=True, h=h, height_range=height_range, bins=bins)\n",
    "    \n",
    "    _set_wedge_ticks(ax, ax_ticks=list(overall_feature_range), wedge_ticks=wedge_ticks)\n",
    "    print(node.split())\n",
    "    print(overall_feature_range)\n",
    "    min_dis = 100000\n",
    "    min_index = -1\n",
    "    for cur_index, cur_value in enumerate(pre_def_ranges[feature_name]):\n",
    "        if abs(cur_value - node.split()) < min_dis:\n",
    "            min_index = cur_index\n",
    "            min_dis = abs(cur_value - node.split())\n",
    "    plt.xticks(ticks=[pre_def_ranges[feature_name][min_index]], labels=[vocabulary[feature_name][min_index]])\n",
    "\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def _class_leaf_viz(node: ShadowDecTreeNode,\n",
    "                    colors: List[str],\n",
    "                    filename: str,\n",
    "                    graph_colors,\n",
    "                    fontname,\n",
    "                    leaftype):\n",
    "    graph_colors = adjust_colors(graph_colors)\n",
    "\n",
    "    minsize = .15\n",
    "    maxsize = 1.3\n",
    "    slope = 0.02\n",
    "    nsamples = node.nsamples()\n",
    "    size = nsamples * slope + minsize\n",
    "    size = min(size, maxsize)\n",
    "    \n",
    "    # we visually need n=1 and n=9 to appear different but diff between 300 and 400 is no big deal\n",
    "    counts = node.class_counts()\n",
    "    prediction = node.prediction_name()\n",
    "\n",
    "    # when using another dataset than the training dataset, some leaves could have 0 samples.\n",
    "    # Trying to make a pie chart will raise some deprecation\n",
    "    if sum(counts) == 0:\n",
    "        return\n",
    "    if leaftype == 'pie':\n",
    "        print(counts)\n",
    "        _draw_piechart(node.id, counts, size=size, colors=colors, filename=filename, label=f\"n={nsamples}\\n{prediction}\",\n",
    "                      graph_colors=graph_colors, fontname=fontname)\n",
    "    elif leaftype == 'barh':\n",
    "        _draw_barh_chart(counts, size=size, colors=colors, filename=filename, label=f\"n={nsamples}\\n{prediction}\",\n",
    "                      graph_colors=graph_colors, fontname=fontname)\n",
    "    else:\n",
    "        raise ValueError(f'Undefined leaftype = {leaftype}')\n",
    "\n",
    "\n",
    "def _regr_split_viz(node: ShadowDecTreeNode,\n",
    "                    X_train: np.ndarray,\n",
    "                    y_train: np.ndarray,\n",
    "                    target_name: str,\n",
    "                    filename,\n",
    "                    y_range,\n",
    "                    ticks_fontsize,\n",
    "                    label_fontsize,\n",
    "                    fontname: str,\n",
    "                    X: np.array,\n",
    "                    highlight_node: bool,\n",
    "                    colors):\n",
    "    colors = adjust_colors(colors)\n",
    "\n",
    "    figsize = (2.5, 1.1)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    feature_name = node.feature_name()\n",
    "    _format_axes(ax, feature_name, target_name if node == node.shadow_tree.root else None, colors, fontsize=label_fontsize, fontname=fontname, ticks_fontsize=ticks_fontsize, grid=False, pad_for_wedge=True)\n",
    "    ax.set_ylim(y_range)\n",
    "\n",
    "    # Get X, y data for all samples associated with this node.\n",
    "    X_feature = X_train[:, node.feature()]\n",
    "    X_feature, y_train = X_feature[node.samples()], y_train[node.samples()]\n",
    "\n",
    "    # only for str categorical features which are str type, int categorical features can work fine as numerical ones\n",
    "    if node.is_categorical_split() and type(X_feature[0]) == str:\n",
    "        # TODO think if the len() should be from all training[feature] data vs only data from this specific node ?\n",
    "        overall_feature_range = (0, len(np.unique(X_feature)) - 1)\n",
    "    else:\n",
    "        overall_feature_range = (np.min(X_train[:, node.feature()]), np.max(X_train[:, node.feature()]))\n",
    "\n",
    "    ax.set_xlim(*overall_feature_range)\n",
    "    xmin, xmax = overall_feature_range\n",
    "\n",
    "    if not node.is_categorical_split():\n",
    "        ax.scatter(X_feature, y_train, s=5, c=colors['scatter_marker'], alpha=colors['scatter_marker_alpha'], lw=.3)\n",
    "        left, right = node.split_samples()\n",
    "        left = y_train[left]\n",
    "        right = y_train[right]\n",
    "        split = node.split()\n",
    "\n",
    "        ax.plot([overall_feature_range[0], split], [np.mean(left), np.mean(left)], '--', color=colors['split_line'],\n",
    "                linewidth=1)\n",
    "        ax.plot([split, split], [*y_range], '--', color=colors['split_line'], linewidth=1)\n",
    "        ax.plot([split, overall_feature_range[1]], [np.mean(right), np.mean(right)], '--', color=colors['split_line'],\n",
    "                linewidth=1)\n",
    "\n",
    "        wedge_ticks = _draw_wedge(ax, x=node.split(), node=node, color=colors['wedge'], is_classifier=False)\n",
    "\n",
    "        if highlight_node:\n",
    "            _ = _draw_wedge(ax, x=X[node.feature()], node=node, color=colors['highlight'], is_classifier=False)\n",
    "\n",
    "        _set_wedge_ticks(ax, ax_ticks=list(overall_feature_range), wedge_ticks=wedge_ticks)\n",
    "\n",
    "    else:\n",
    "        left_index, right_index = node.split_samples()\n",
    "        tw = (xmax - xmin) * .018\n",
    "\n",
    "        ax.set_xlim(overall_feature_range[0] - tw, overall_feature_range[1] + tw)\n",
    "        ax.scatter(X_feature[left_index], y_train[left_index], s=5, c=colors[\"categorical_split_left\"],\n",
    "                   alpha=colors['scatter_marker_alpha'], lw=.3)\n",
    "        ax.scatter(X_feature[right_index], y_train[right_index], s=5, c=colors[\"categorical_split_right\"],\n",
    "                   alpha=colors['scatter_marker_alpha'], lw=.3)\n",
    "\n",
    "        ax.plot([xmin - tw, xmax + tw], [np.mean(y_train[left_index]), np.mean(y_train[left_index])], '--',\n",
    "                color=colors[\"categorical_split_left\"],\n",
    "                linewidth=1)\n",
    "        ax.plot([xmin - tw, xmax + tw], [np.mean(y_train[right_index]), np.mean(y_train[right_index])], '--',\n",
    "                color=colors[\"categorical_split_right\"],\n",
    "                linewidth=1)\n",
    "\n",
    "        # no wedge ticks for categorical split, just the x_ticks in case the categorical value is not a string\n",
    "        # if it's a string, then the xticks label will be handled automatically by ax.scatter plot\n",
    "        if type(X_feature[0]) is not str:\n",
    "            ax.set_xticks(np.unique(np.concatenate((X_feature, np.asarray(overall_feature_range)))))\n",
    "\n",
    "        if highlight_node:\n",
    "            highlight_value = X[node.feature()]\n",
    "            if type(X_feature[0]) is str:\n",
    "                plt.draw()\n",
    "                # get the label text and its position from the figure\n",
    "                label_index = dict([(label.get_text(), label.get_position()[0]) for label in ax.get_xticklabels()])\n",
    "                highlight_value = label_index[X[node.feature()]]\n",
    "            _ = _draw_wedge(ax, x=highlight_value, node=node, color=colors['highlight'], is_classifier=False)\n",
    "\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def _regr_leaf_viz(node: ShadowDecTreeNode,\n",
    "                   y: (pd.Series, np.ndarray),\n",
    "                   target_name,\n",
    "                   filename: str,\n",
    "                   y_range,\n",
    "                   precision,\n",
    "                   label_fontsize: int,\n",
    "                   ticks_fontsize: int,\n",
    "                   fontname: str,\n",
    "                   colors):\n",
    "    colors = adjust_colors(colors)\n",
    "\n",
    "    samples = node.samples()\n",
    "    y = y[samples]\n",
    "\n",
    "    figsize = (.75, .8)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    m = node.prediction()\n",
    "\n",
    "    _format_axes(ax, None, None, colors, fontsize=label_fontsize, fontname=fontname, ticks_fontsize=ticks_fontsize, grid=False)\n",
    "    ax.set_ylim(y_range)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.set_xticks([])\n",
    "\n",
    "    ticklabelpad = plt.rcParams['xtick.major.pad']\n",
    "    ax.annotate(f\"{target_name}={myround(m, precision)}\\nn={len(y)}\",\n",
    "                xy=(.5, 0), xytext=(.5, -.5 * ticklabelpad), ha='center', va='top',\n",
    "                xycoords='axes fraction', textcoords='offset points',\n",
    "                fontsize=label_fontsize, fontname=fontname, color=colors['axis_label'])\n",
    "\n",
    "    mu = .5\n",
    "    sigma = .08\n",
    "    X = np.random.normal(mu, sigma, size=len(y))\n",
    "    ax.set_xlim(0, 1)\n",
    "    alpha = colors['scatter_marker_alpha']  # was .25\n",
    "\n",
    "    ax.scatter(X, y, s=5, c=colors['scatter_marker'], alpha=alpha, lw=.3)\n",
    "    ax.plot([0, len(node.samples())], [m, m], '--', color=colors['split_line'], linewidth=1)\n",
    "\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def _draw_legend(shadow_tree, target_name, filename, colors, fontname):\n",
    "    colors = adjust_colors(colors)\n",
    "    n_classes = shadow_tree.nclasses()\n",
    "    class_values = shadow_tree.classes()\n",
    "    class_names = shadow_tree.class_names\n",
    "    color_values = colors['classes'][n_classes]\n",
    "    color_map = {v: color_values[i] for i, v in enumerate(class_values)}\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(1, 1))\n",
    "\n",
    "    add_classifier_legend(ax, class_names, class_values, facecolors=color_map, target_name=target_name,\n",
    "                          colors=colors, fontsize=10, fontname=fontname)\n",
    "\n",
    "    ax.set_xlim(0, 20)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def _draw_piechart(node_id, counts, size, colors, filename, label, fontname, graph_colors):\n",
    "    graph_colors = adjust_colors(graph_colors)\n",
    "    n_nonzero = np.count_nonzero(counts)\n",
    "\n",
    "    if n_nonzero != 0:\n",
    "        i = np.nonzero(counts)[0][0]\n",
    "        if n_nonzero == 1:\n",
    "            counts = [counts[i]]\n",
    "            colors = [colors[i]]\n",
    "\n",
    "    tweak = size * .01\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(size, size))\n",
    "    ax.axis('equal')\n",
    "    ax.set_xlim(0, size - 10 * tweak)\n",
    "    ax.set_ylim(0, size - 10 * tweak)\n",
    "    # frame=True needed for some reason to fit pie properly (ugh)\n",
    "    # had to tweak the crap out of this to get tight box around piechart :(\n",
    "    wedges, _ = ax.pie(counts, center=(size / 2 - 6 * tweak, size / 2 - 6 * tweak), radius=size / 2, colors=colors,\n",
    "                       shadow=False, frame=True)\n",
    "    for w in wedges:\n",
    "        w.set_linewidth(.5)\n",
    "        w.set_edgecolor(graph_colors['pie'])\n",
    "    Pie_output = 1\n",
    "    if node_id in leaf_explainations:\n",
    "        Comments = leaf_explainations[node_id]\n",
    "        # if label is not None:\n",
    "        #     ax.text(size / 2 - 6 * tweak, -10 * tweak, label,\n",
    "        #             horizontalalignment='center',\n",
    "        #             verticalalignment='top',\n",
    "        #             fontsize=9, color=graph_colors['text'], fontname=fontname)\n",
    "    elif compute_entropy(counts) > max_tolerate_entropy:\n",
    "        Comments = \"Impure.\" + newline\n",
    "        Comments += f\"Entropy(impurity): {compute_entropy(counts):.2f}.\" + newline\n",
    "        \n",
    "    else:\n",
    "        Comments = \"Rare.\" + newline\n",
    "        Comments += f\"n = {sum(counts)}.\" + newline\n",
    "\n",
    "    #### \n",
    "    # plt.text(0, -10 * tweak, Comments, va='top', fontsize=12, color=graph_colors['text'], fontname=fontname)\n",
    "    ax.axis('off')\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "\n",
    "    plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(1, 1))\n",
    "    # plt.text(0, -10 * tweak, Comments, va='top', fontsize=12, color=graph_colors['text'], fontname=fontname)\n",
    "    plt.text(0, 0, Comments, va='top', fontsize=12, color=graph_colors['text'], fontname=fontname)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(filename+\".svg\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def _draw_barh_chart(counts, size, colors, filename, label, fontname, graph_colors):\n",
    "    graph_colors = adjust_colors(graph_colors)\n",
    "    n_nonzero = np.count_nonzero(counts)\n",
    "\n",
    "    if n_nonzero != 0:\n",
    "        i = np.nonzero(counts)[0][0]\n",
    "        if n_nonzero == 1:\n",
    "            counts = [counts[i]]\n",
    "            colors = [colors[i]]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(size, 0.2))\n",
    "\n",
    "    data_cum = 0\n",
    "    for i in range(len(counts)):\n",
    "        width = counts[i]\n",
    "        plt.barh(0, width, left=data_cum, color=colors[i], height=1, edgecolor=graph_colors['rect_edge'], linewidth=0.5)\n",
    "        data_cum += width\n",
    "\n",
    "    # No axes\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.tick_params(axis='both', which='both', top=False, bottom=False, left=False, right=False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_xticks([])\n",
    "\n",
    "    if label is not None:\n",
    "        ax.text(sum(counts) / 2, -1, label,\n",
    "                horizontalalignment='center',\n",
    "                verticalalignment='top',\n",
    "                fontsize=9, color=graph_colors['text'], fontname=fontname)\n",
    "\n",
    "    plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def _prop_size(n, counts, output_range=(0.00, 0.3)):\n",
    "    min_samples = min(counts)\n",
    "    max_samples = max(counts)\n",
    "    sample_count_range = max_samples - min_samples\n",
    "\n",
    "    if sample_count_range > 0:\n",
    "        zero_to_one = (n - min_samples) / sample_count_range\n",
    "        return zero_to_one * (output_range[1] - output_range[0]) + output_range[0]\n",
    "    else:\n",
    "        return output_range[0]\n",
    "\n",
    "\n",
    "def _get_num_bins(histtype, n_classes):\n",
    "    bins = NUM_BINS[n_classes]\n",
    "    if histtype == 'barstacked':\n",
    "        bins *= 2\n",
    "    return bins\n",
    "\n",
    "\n",
    "def _get_leaf_target_input(shadow_tree: ShadowDecTree, precision: int):\n",
    "    x = []\n",
    "    y = []\n",
    "    means = []\n",
    "    means_range = []\n",
    "    x_labels = []\n",
    "    sigma = .05\n",
    "    for i, node in enumerate(shadow_tree.leaves):\n",
    "        leaf_index_sample = node.samples()\n",
    "        leaf_target = shadow_tree.y_train[leaf_index_sample]\n",
    "        leaf_target_mean = node.prediction()\n",
    "        np.random.seed(0)  # generate the same list of random values for each call\n",
    "        X = np.random.normal(i, sigma, size=len(leaf_target))\n",
    "\n",
    "        x.extend(X)\n",
    "        y.extend(leaf_target)\n",
    "        means.append([leaf_target_mean, leaf_target_mean])\n",
    "        means_range.append([i - (sigma * 3), i + (sigma * 3)])\n",
    "        x_labels.append(f\"{myround(leaf_target_mean, precision)}\")\n",
    "\n",
    "    return x, y, means, means_range, x_labels\n",
    "\n",
    "\n",
    "def _ctreeviz_univar(shadow_tree,\n",
    "                     fontsize, ticks_fontsize, fontname, nbins,\n",
    "                     gtype,\n",
    "                     show,\n",
    "                     colors,\n",
    "                     feature,\n",
    "                     figsize,\n",
    "                     ax):\n",
    "    if ax is None:\n",
    "        if figsize:\n",
    "            fig, ax = plt.subplots(figsize=figsize)\n",
    "        else:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "    featidx = shadow_tree.feature_names.index(feature)\n",
    "    X_train = shadow_tree.X_train\n",
    "    y_train = shadow_tree.y_train\n",
    "    colors = adjust_colors(colors)\n",
    "    n_classes = shadow_tree.nclasses()\n",
    "    overall_feature_range = (np.min(X_train[:,featidx]), np.max(X_train[:,featidx]))\n",
    "    class_values = shadow_tree.classes()\n",
    "    color_values = colors['classes'][n_classes]\n",
    "    color_map = {v: color_values[i] for i, v in enumerate(class_values)}\n",
    "    X_colors = [color_map[cl] for cl in class_values]\n",
    "\n",
    "    # if np.numeric(X_train[:,featidx])\n",
    "    if not is_numeric(X_train[:,featidx]):\n",
    "        raise ValueError(f\"ctree_feature_space only supports numeric feature spaces\")\n",
    "\n",
    "    _format_axes(ax, shadow_tree.feature_names[featidx], 'Count' if gtype=='barstacked' else None,\n",
    "                 colors, fontsize, fontname, ticks_fontsize=ticks_fontsize, grid=False)\n",
    "\n",
    "    X_hist = [X_train[y_train == cl,featidx] for cl in class_values]\n",
    "\n",
    "    if gtype == 'barstacked':\n",
    "        bins = np.linspace(start=overall_feature_range[0], stop=overall_feature_range[1],\n",
    "                           num=nbins, endpoint=True)\n",
    "        hist, bins, barcontainers = ax.hist(X_hist,\n",
    "                                            color=X_colors,\n",
    "                                            align='mid',\n",
    "                                            histtype='barstacked',\n",
    "                                            bins=bins,\n",
    "                                            label=shadow_tree.class_names)\n",
    "\n",
    "        for patch in barcontainers:\n",
    "            for rect in patch.patches:\n",
    "                rect.set_linewidth(.5)\n",
    "                rect.set_edgecolor(colors['rect_edge'])\n",
    "        ax.set_xlim(*overall_feature_range)\n",
    "        ax.set_xticks(overall_feature_range)\n",
    "        ax.set_yticks([0, max([max(h) for h in hist])])\n",
    "    elif gtype == 'strip':\n",
    "        # user should pass in short and wide fig\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        sigma = .013\n",
    "        mu = .08\n",
    "        class_step = .08\n",
    "        dot_w = 20\n",
    "        ax.set_ylim(0, mu + n_classes * class_step)\n",
    "        for i, bucket in enumerate(X_hist):\n",
    "            y_noise = np.random.normal(mu + i * class_step, sigma, size=len(bucket))\n",
    "            ax.scatter(bucket, y_noise, alpha=colors['scatter_marker_alpha'], marker='o', s=dot_w, c=color_map[i],\n",
    "                       edgecolors=colors['scatter_edge'], lw=.3)\n",
    "    else:\n",
    "        raise ValueError(f'Unrecognized gtype = {gtype}!')\n",
    "\n",
    "    splits = [node.split() for node in shadow_tree.internal if node.feature()==featidx]\n",
    "    splits = sorted(splits)\n",
    "\n",
    "    if 'preds' in show:  # this gets the horiz bars showing prediction region\n",
    "        pred_box_height = .07 * (ax.get_ylim()[1] - ax.get_ylim()[0])\n",
    "        bins = [ax.get_xlim()[0]] + splits + [ax.get_xlim()[1]]\n",
    "        for i in range(len(bins) - 1):\n",
    "            left = bins[i]\n",
    "            right = bins[i + 1]\n",
    "            inrange = y_train[(X_train >= left) & (X_train <= right)]\n",
    "            if len(inrange) == 0:\n",
    "                continue\n",
    "            values, counts = np.unique(inrange, return_counts=True)\n",
    "            pred = values[np.argmax(counts)]\n",
    "            rect = patches.Rectangle((left, -2*pred_box_height), (right - left), pred_box_height, linewidth=.3,\n",
    "                                     edgecolor=colors['edge'], facecolor=color_map[pred])\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    if 'legend' in show:\n",
    "        add_classifier_legend(ax, shadow_tree.class_names, class_values, color_map, shadow_tree.target_name, colors,\n",
    "                              fontname=fontname)\n",
    "\n",
    "    if 'title' in show:\n",
    "        accur = shadow_tree.get_score()\n",
    "        title = f\"Classifier Tree Depth {shadow_tree.get_max_depth()}, Training Accuracy={accur * 100:.2f}%\"\n",
    "        ax.set_title(title, fontsize=fontsize, color=colors['title'])\n",
    "\n",
    "    if 'splits' in show:\n",
    "        split_heights = [*ax.get_ylim()]\n",
    "        for split in splits:\n",
    "            ax.plot([split, split], split_heights, '--', color=colors['split_line'], linewidth=1)\n",
    "\n",
    "\n",
    "def _ctreeviz_bivar(shadow_tree, fontsize, ticks_fontsize, fontname, show,\n",
    "                    colors,\n",
    "                    features,\n",
    "                    figsize,\n",
    "                    ax):\n",
    "    \"\"\"\n",
    "    Show tesselated 2D feature space for bivariate classification tree. X_train can\n",
    "    have lots of features but features lists indexes of 2 features to train tree with.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        if figsize:\n",
    "            fig, ax = plt.subplots(figsize=figsize)\n",
    "        else:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "    featidx = [shadow_tree.feature_names.index(f) for f in features]\n",
    "    X_train = shadow_tree.X_train\n",
    "    y_train = shadow_tree.y_train\n",
    "    colors = adjust_colors(colors)\n",
    "    tessellation = tessellate(shadow_tree.root, X_train, featidx)\n",
    "    n_classes = shadow_tree.nclasses()\n",
    "    class_values = shadow_tree.classes()\n",
    "    color_values = colors['classes'][n_classes]\n",
    "    color_map = {v: color_values[i] for i, v in enumerate(class_values)}\n",
    "\n",
    "    if not is_numeric(X_train[:,featidx[0]]) or not is_numeric(X_train[:,featidx[1]]):\n",
    "        raise ValueError(f\"ctree_feature_space only supports numeric feature spaces\")\n",
    "\n",
    "    dot_w = 25\n",
    "    X_hist = [X_train[y_train == cl] for cl in class_values]\n",
    "    for i, h in enumerate(X_hist):\n",
    "        ax.scatter(h[:, featidx[0]], h[:, featidx[1]], marker='o', s=dot_w, c=color_map[i],\n",
    "                   edgecolors=colors['scatter_edge'], lw=.3)\n",
    "\n",
    "    _format_axes(ax, shadow_tree.feature_names[featidx[0]], shadow_tree.feature_names[featidx[1]],\n",
    "                 colors, fontsize, fontname, ticks_fontsize=ticks_fontsize, grid=False)\n",
    "\n",
    "    if 'splits' in show:\n",
    "        plt.draw()\n",
    "        for node, bbox in tessellation:\n",
    "            x = bbox[0]\n",
    "            y = bbox[1]\n",
    "            w = bbox[2] - bbox[0]\n",
    "            h = bbox[3] - bbox[1]\n",
    "            rect = patches.Rectangle((x, y), w, h, angle=0, linewidth=.3, alpha=colors['tessellation_alpha'],\n",
    "                                     edgecolor=colors['rect_edge'], facecolor=color_map[node.prediction()])\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    if 'legend' in show:\n",
    "        add_classifier_legend(ax, shadow_tree.class_names, class_values, color_map, shadow_tree.target_name, colors,\n",
    "                              fontname=fontname)\n",
    "\n",
    "    if 'title' in show:\n",
    "        accur = shadow_tree.get_score()\n",
    "        title = f\"Classifier Tree Depth {shadow_tree.get_max_depth()}, Training Accuracy={accur * 100:.2f}%\"\n",
    "        ax.set_title(title, fontsize=fontsize, color=colors['title'], )\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _rtreeviz_univar(shadow_tree, fontsize, ticks_fontsize, fontname, show,\n",
    "                     split_linewidth, mean_linewidth, markersize, colors,\n",
    "                     feature,\n",
    "                     figsize, ax):\n",
    "    featidx = shadow_tree.feature_names.index(feature)\n",
    "    X_train = shadow_tree.X_train\n",
    "    y_train = shadow_tree.y_train\n",
    "    if X_train is None or y_train is None:\n",
    "        raise ValueError(f\"X_train and y_train must not be none\")\n",
    "\n",
    "    if not is_numeric(X_train[:,featidx]):\n",
    "       raise ValueError(f\"rtree_feature_space only supports numeric feature spaces\")\n",
    "\n",
    "    if ax is None:\n",
    "        if figsize:\n",
    "            fig, ax = plt.subplots(figsize=figsize)\n",
    "        else:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "    colors = adjust_colors(colors)\n",
    "\n",
    "    y_range = (min(y_train), max(y_train))  # same y axis for all\n",
    "    overall_feature_range = (np.min(X_train[:,featidx]), np.max(X_train[:,featidx]))\n",
    "\n",
    "    splits = []\n",
    "    for node in shadow_tree.internal:\n",
    "        if node.feature()==featidx:\n",
    "            splits.append(node.split())\n",
    "    splits = sorted(splits)\n",
    "    bins = [overall_feature_range[0]] + splits + [overall_feature_range[1]]\n",
    "\n",
    "    means = []\n",
    "    for i in range(len(bins) - 1):\n",
    "        left = bins[i]\n",
    "        right = bins[i + 1]\n",
    "        inrange = y_train[(X_train[:,featidx] >= left) & (X_train[:,featidx] <= right)]\n",
    "        means.append(np.mean(inrange))\n",
    "\n",
    "    ax.scatter(X_train[:,featidx], y_train, marker='o', alpha=colors['scatter_marker_alpha'], c=colors['scatter_marker'],\n",
    "               s=markersize,\n",
    "               edgecolor=colors['scatter_edge'], lw=.3)\n",
    "\n",
    "    if 'splits' in show:\n",
    "        for split in splits:\n",
    "            ax.plot([split, split], [*y_range], '--', color=colors['split_line'], linewidth=split_linewidth)\n",
    "\n",
    "        prevX = overall_feature_range[0]\n",
    "        for i, m in enumerate(means):\n",
    "            split = overall_feature_range[1]\n",
    "            if i < len(splits):\n",
    "                split = splits[i]\n",
    "            ax.plot([prevX, split], [m, m], '-', color=colors['mean_line'], linewidth=mean_linewidth)\n",
    "            prevX = split\n",
    "\n",
    "    _format_axes(ax, shadow_tree.feature_names[featidx], shadow_tree.target_name, colors, fontsize, fontname, ticks_fontsize=ticks_fontsize, grid=False)\n",
    "\n",
    "    if 'title' in show:\n",
    "        title = f\"Regression Tree Depth {shadow_tree.get_max_depth()}, Samples per Leaf {shadow_tree.get_min_samples_leaf()},\\nTraining $R^2$={shadow_tree.get_score()}\"\n",
    "        ax.set_title(title, fontsize=fontsize, color=colors['title'])\n",
    "\n",
    "\n",
    "def _rtreeviz_bivar_heatmap(shadow_tree, fontsize, ticks_fontsize, fontname,\n",
    "                            show,\n",
    "                            n_colors_in_map,\n",
    "                            colors,\n",
    "                            markersize,\n",
    "                            features,\n",
    "                            figsize,\n",
    "                            ax):\n",
    "    \"\"\"\n",
    "    Show tesselated 2D feature space for bivariate regression tree. X_train can\n",
    "    have lots of features but features lists indexes of 2 features to train tree with.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        if figsize:\n",
    "            fig, ax = plt.subplots(figsize=figsize)\n",
    "        else:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "    X_train = shadow_tree.X_train\n",
    "    y_train = shadow_tree.y_train\n",
    "\n",
    "    colors = adjust_colors(colors)\n",
    "\n",
    "    y_lim = np.min(y_train), np.max(y_train)\n",
    "    y_range = y_lim[1] - y_lim[0]\n",
    "    color_map = [rgb2hex(c.rgb, force_long=True) for c in\n",
    "                 Color(colors['color_map_min']).range_to(Color(colors['color_map_max']),\n",
    "                                                         n_colors_in_map)]\n",
    "    featidx = [shadow_tree.feature_names.index(f) for f in features]\n",
    "\n",
    "    if not is_numeric(X_train[:,featidx[0]]) or not is_numeric(X_train[:,featidx[1]]):\n",
    "        raise ValueError(f\"rtree_feature_space only supports numeric feature spaces\")\n",
    "\n",
    "    tessellation = tessellate(shadow_tree.root, X_train, featidx)\n",
    "\n",
    "    for node, bbox in tessellation:\n",
    "        pred = node.prediction()\n",
    "        color = color_map[int(((pred - y_lim[0]) / y_range) * (n_colors_in_map - 1))]\n",
    "        x = bbox[0]\n",
    "        y = bbox[1]\n",
    "        w = bbox[2] - bbox[0]\n",
    "        h = bbox[3] - bbox[1]\n",
    "        rect = patches.Rectangle((x, y), w, h, angle=0, linewidth=.3, alpha=colors['tessellation_alpha'],\n",
    "                                 edgecolor=colors['edge'], facecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    color_map = [color_map[int(((y - y_lim[0]) / y_range) * (n_colors_in_map - 1))] for y in y_train]\n",
    "    ax.scatter(X_train[:, featidx[0]], X_train[:, featidx[1]], marker='o', c=color_map,\n",
    "               edgecolor=colors['scatter_edge'], lw=.3, s=markersize)\n",
    "\n",
    "    _format_axes(ax, shadow_tree.feature_names[featidx[0]], shadow_tree.feature_names[featidx[1]],\n",
    "                 colors, fontsize, fontname, ticks_fontsize=ticks_fontsize, grid=False)\n",
    "\n",
    "    if 'title' in show:\n",
    "        accur = shadow_tree.get_score()\n",
    "        title = f\"Regression Tree Depth {shadow_tree.get_max_depth()}, Training $R^2$={accur:.3f}\"\n",
    "        ax.set_title(title, fontsize=fontsize, color=colors['title'])\n",
    "\n",
    "\n",
    "def _rtreeviz_bivar_3D(shadow_tree, fontsize, ticks_fontsize, fontname,\n",
    "                       azim, elev, dist,\n",
    "                       show, colors, markersize,\n",
    "                       n_colors_in_map,\n",
    "                       features,\n",
    "                       figsize, ax):\n",
    "    X_train = shadow_tree.X_train\n",
    "    y_train = shadow_tree.y_train\n",
    "\n",
    "    if ax is None:\n",
    "        if figsize:\n",
    "            fig = plt.figure(figsize=figsize)\n",
    "        else:\n",
    "            fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    colors = adjust_colors(colors)\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.dist = dist\n",
    "\n",
    "    def y_to_color_index(y):\n",
    "        y_range = y_lim[1] - y_lim[0]\n",
    "        return int(((y - y_lim[0]) / y_range) * (n_colors_in_map - 1))\n",
    "\n",
    "    def plane(node, bbox, color_spectrum):\n",
    "        x = np.linspace(bbox[0], bbox[2], 2)\n",
    "        y = np.linspace(bbox[1], bbox[3], 2)\n",
    "        xx, yy = np.meshgrid(x, y)\n",
    "        z = np.full(xx.shape, node.prediction())\n",
    "        ax.plot_surface(xx, yy, z, alpha=colors['tessellation_alpha_3D'], shade=False,\n",
    "                        color=color_spectrum[y_to_color_index(node.prediction())],\n",
    "                        edgecolor=colors['edge'], lw=.3)\n",
    "\n",
    "    y_lim = np.min(y_train), np.max(y_train)\n",
    "    color_spectrum = Color(colors['color_map_min']).range_to(Color(colors['color_map_max']), n_colors_in_map)\n",
    "    color_spectrum = [rgb2hex(c.rgb, force_long=True) for c in color_spectrum]\n",
    "    y_colors = [color_spectrum[y_to_color_index(y)] for y in y_train]\n",
    "\n",
    "    featidx = [shadow_tree.feature_names.index(f) for f in features]\n",
    "\n",
    "    if not is_numeric(X_train[:,featidx[0]]) or not is_numeric(X_train[:,featidx[1]]):\n",
    "        raise ValueError(f\"rtree_feature_space3D only supports numeric feature spaces\")\n",
    "\n",
    "    x, y, z = X_train[:, featidx[0]], X_train[:, featidx[1]], y_train\n",
    "    tessellation = tessellate(shadow_tree.root, X_train, featidx)\n",
    "\n",
    "    for node, bbox in tessellation:\n",
    "        plane(node, bbox, color_spectrum)\n",
    "\n",
    "    ax.scatter(x, y, z, marker='o', alpha=colors['scatter_marker_alpha'], edgecolor=colors['scatter_edge'],\n",
    "               lw=.3, c=y_colors, s=markersize)\n",
    "\n",
    "    _format_axes(ax, shadow_tree.feature_names[featidx[0]], shadow_tree.feature_names[featidx[1]], colors, fontsize, fontname, ticks_fontsize=ticks_fontsize, grid=False)\n",
    "    ax.set_zlabel(f\"{shadow_tree.target_name}\", fontsize=fontsize, fontname=fontname, color=colors['axis_label'])\n",
    "\n",
    "    if 'title' in show:\n",
    "        accur = shadow_tree.get_score()\n",
    "        title = f\"Regression Tree Depth {shadow_tree.get_max_depth()}, Training $R^2$={accur:.3f}\"\n",
    "        ax.set_title(title, fontsize=fontsize, color=colors['title'])\n",
    "\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f0d8386-4b3e-464f-95ce-e232866d1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_with_only_main(model,\n",
    "          X_train,\n",
    "          y_train,\n",
    "          tree_index: int = None,\n",
    "          feature_names: List[str] = None,\n",
    "          target_name: str = None,\n",
    "          class_names: (List[str], Mapping[int, str]) = None) -> DTreeVizAPI:\n",
    "    \"\"\"\n",
    "    Given a decision tree-based model from a supported decision-tree library, training data, and\n",
    "    information about the data, create a model adaptor that provides a consistent interface for\n",
    "    the overall dtreeviz lib to the various supported tree libraries. Call methods such as v.view(),\n",
    "    v.explain_prediction_path(), v.rtree_feature_space3D() on the returned adaptor v.\n",
    "\n",
    "    :param model: A tree-based model from a supportive decision tree library, such as sklearn, XGBoost, and TensorFlow.\n",
    "    :param X_train: Features used to train model; 2D array-like object of shape (n_samples, n_features).\n",
    "    :param y_train: Classifier or regressor target used to train model; 1D array-like object of shape (n_samples, 1).\n",
    "    :param tree_index: Index (from 0) of tree if model is an ensemble of trees like a random forest.\n",
    "    :param feature_names: Names of features in the same order of X_train.\n",
    "    :param target_name: What is the (string) name of the target variable; e.g., for a house price regressor, this might be \"price\".\n",
    "    :param class_names: For classifiers, what are the names associated with the labels?\n",
    "    :return: a DTreeVizAPI object that provides the main API for dtreeviz (version 2.0.0+);\n",
    "             e.g., call the view() method on the return object to display it in a notebook.\n",
    "    \"\"\"\n",
    "    shadow_tree = ShadowDecTree.get_shadow_tree(model, X_train, y_train,\n",
    "                                                feature_names, target_name, class_names,\n",
    "                                                tree_index)\n",
    "    dtreeviz_model = DTreeVizAPI(shadow_tree)\n",
    "    return dtreeviz_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "449075e9-2ed1-4899-b295-41bd76c8bc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/binshuaiwang/PycharmProjects/XAI_project/venv/lib/python3.10/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174.5\n",
      "(0.0, 547.0)\n",
      "316.5\n",
      "(0.0, 547.0)\n",
      "282.5\n",
      "(0.0, 547.0)\n",
      "342.5\n",
      "(0.0, 547.0)\n",
      "405.0\n",
      "(0.0, 547.0)\n",
      "-928.0\n",
      "(-4480.0, 1536.0)\n",
      "96.89522552490234\n",
      "(0.0, 582.5260561488991)\n",
      "280.0\n",
      "(0.0, 547.0)\n",
      "177.0173568725586\n",
      "(0.0, 582.5260561488991)\n",
      "308.5\n",
      "(0.0, 547.0)\n",
      "32.722442626953125\n",
      "(0.0, 582.5260561488991)\n",
      "311.5\n",
      "(0.0, 547.0)\n",
      "268.5\n",
      "(0.0, 547.0)\n",
      "-416.0\n",
      "(-4480.0, 1536.0)\n",
      "-672.0\n",
      "(-4480.0, 1536.0)\n",
      "0.037660421803593636\n",
      "(0.0, 582.5260561488991)\n",
      "0.004970981739461422\n",
      "(0.0, 582.5260561488991)\n",
      "0.5\n",
      "(0.0, 547.0)\n",
      "441.5\n",
      "(0.0, 547.0)\n",
      "119.64925384521484\n",
      "(0.0, 582.5260561488991)\n",
      "521.0\n",
      "(0.0, 547.0)\n",
      "414.5\n",
      "(0.0, 547.0)\n",
      "422.5\n",
      "(0.0, 547.0)\n",
      "515.5328674316406\n",
      "(0.0, 582.5260561488991)\n",
      "456.9036102294922\n",
      "(0.0, 582.5260561488991)\n",
      "224.0\n",
      "(-4480.0, 1536.0)\n",
      "132.07074737548828\n",
      "(0.0, 582.5260561488991)\n",
      "-160.0\n",
      "(-4480.0, 1536.0)\n",
      "[   0    0    0 1132    1]\n",
      "[   0    0    7 3155   88]\n",
      "[ 614    0    2 1077   30]\n",
      "[ 414    0    7 4708   41]\n",
      "[ 0  0  0 60 11]\n",
      "[   0    0    3 4158   34]\n",
      "[ 0  1  0 55 13]\n",
      "[  0   0  10 586  16]\n",
      "[  0   0   3 608  57]\n",
      "[42  0 11 65 11]\n",
      "[  0   1  17 963  47]\n",
      "[ 0  0 13 28  9]\n",
      "[  0   0  13 115  74]\n",
      "[  0   0  11 269  18]\n",
      "[23  0 17 13  8]\n",
      "[  0   8  40 159  63]\n",
      "[   0    0   15    0 1309]\n",
      "[   0    0    0    0 1161]\n",
      "[  0   0 598   0  73]\n",
      "[  229    77  4130   294 10338]\n",
      "[ 0  0 16  6  3]\n",
      "[   0    0 1478    0    0]\n",
      "[ 0  0  0  0 68]\n",
      "[ 0  0  0  8 12]\n",
      "[   0    0 2890   21  600]\n",
      "[    0    10 10915    24   214]\n",
      "[  0   0   0   0 454]\n",
      "[ 0  6  0  0 15]\n",
      "[  0 677   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "viz_model = model_with_only_main(dtree_small_entropy, X_df, Y, \n",
    "                target_name=\"target\",\n",
    "                feature_names=list(X_df.columns),\n",
    "                class_names=target_names)\n",
    "\n",
    "# viz_model.ctree_leaf_distributions(display_type = \"text\")\n",
    "# viz_model.node_stats(node_id=10)\n",
    "v = viz_model.view()     # render as SVG into internal object \n",
    "v                 # pop up window\n",
    "v.save(f\"{data_name}_all_{Target_category_name}_{criterion}_dtree\" + f\"max_depth:{max_depth}\"  + \".svg\")  # optionally save as svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731b764-553b-4202-8eba-4303c8e9960c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b5a9f-c4de-4223-b897-abfeb9010371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
